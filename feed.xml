<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://emanubuc.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://emanubuc.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-18T21:46:59+00:00</updated><id>https://emanubuc.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html"></title><link href="https://emanubuc.github.io/blog/2026/2025-07-11-Online_latex_editor/" rel="alternate" type="text/html" title=""/><published>2026-01-18T21:46:59+00:00</published><updated>2026-01-18T21:46:59+00:00</updated><id>https://emanubuc.github.io/blog/2026/2025-07-11-Online_latex_editor</id><content type="html" xml:base="https://emanubuc.github.io/blog/2026/2025-07-11-Online_latex_editor/"><![CDATA[<h1 id="online-latex-editors-and-collaboration-tools-for-academic-writing-features-pricing-and-security">Online LaTeX Editors and Collaboration Tools for Academic Writing: Features, Pricing, and Security</h1> <p><strong>Date:</strong> September 11, 2025</p> <h2 id="abstract">Abstract</h2> <p>This post provides a comprehensive analysis of online LaTeX editors and collaboration tools designed for academic writing. We examine leading platforms including Overleaf, Papeeria, Authorea, and other alternatives, comparing their features, pricing models, and collaborative capabilities. Special attention is given to available AI powered writing assistant and to data security and privacy compliance (particularly GDPR) for protecting sensitive academic content. Users should choose the solution that best fits requirements and budget constrains. Our analysis reveals that while in online LaTeX editors offer significant advantages for collaborative academic writing, institutions must carefully evaluate security implications when handling sensitive research data. Different set of features are offered across a tiered pricing model, ranging from robust free plans to professional tiers costing up to 40 ‚Ç¨/month.</p> <h2 id="introduction">Introduction</h2> <p>LaTeX has become the gold standard for academic and scientific document preparation, especially in fields requiring complex notation, precise formatting, and strong bibliographic management. The evolution from desktop to cloud-based collaborative platforms has revolutionized research workflows.</p> <p>Online LaTeX editors combine powerful typesetting with modern collaboration tools, enabling real-time editing, version control, and sharing across teams.</p> <h2 id="online-latex-editors">Online LaTeX Editors</h2> <p>The most popular and widely used online LaTeX editor is <em>Overleaf</em>, known for its collaborative features, real-time editing, document sharing, and extensive template library, making it a standard tool for academic and technical writing worldwide. Other popular online editors include <em>Papeeria</em> (https://papeeria.com) and <em>Authorea</em> (https://www.authorea.com). Beyond the most popular and established platforms, a new generation of online LaTeX editors is gaining traction. Also cloud-based managed computation platforms such as <em>CoCalc</em> (https://cocalc.com/) that include a LaTeX editor should be considered.</p> <h3 id="getting-started">Getting started</h3> <p>All online LaTeX featured a very friendly on-boarding process that allow the user to start writing in less than 2 minutes. Only an e-mail address is required activate a free plan instance. <em>CoCalc</em>, due to the different nature of the platform, require some more effort for the initial setup and do not provide a free plan.</p> <h3 id="overleaf">Overleaf</h3> <p>Overleaf is the dominant player, now serving over 17 million users. After merging with ShareLaTeX, it offers:</p> <ul> <li> <p>Real-time collaborative editing</p> </li> <li> <p>400+ academic templates</p> </li> <li> <p>Integrated PDF view and SyncTeX support</p> </li> <li> <p>Advanced bibliography management</p> </li> <li> <p>Git integration and document history</p> </li> <li> <p>Track changes and commenting system</p> </li> </ul> <p>Integration includes reference management tools (Zotero, Mendeley) and services like Dropbox and GitHub. Overleaf partners with Paperpal for grammar checking tailored to LaTeX documents.</p> <h3 id="papeeria">Papeeria</h3> <p>Papeeria emphasizes simplicity but retains robust LaTeX functionality. Unique features include:</p> <ul> <li> <p>Real-time collaboration</p> </li> <li> <p>Integrated plotting with gnuplot</p> </li> <li> <p>Version control and Git sync</p> </li> <li> <p>Auto-compiling on paid tiers</p> </li> <li> <p>Mendeley integration for references</p> </li> </ul> <p>Pricing: Free plan includes unlimited public projects, the Delta plan (USD 5/month) unlocks private projects and advanced features. Verified students get Delta features free.</p> <h3 id="authorea">Authorea</h3> <p>Authorea, acquired by Wiley, goes beyond LaTeX and supports:</p> <ul> <li> <p>Data integration (embedding datasets, plotting)</p> </li> <li> <p>Executable code blocks and Jupyter notebooks</p> </li> <li> <p>Journal-specific formatting for 40+ publishers</p> </li> <li> <p>Pre-print workflow integration</p> </li> </ul> <p>Best for computational research requiring close integration between text and data.</p> <h3 id="emerging-alternatives">Emerging Alternatives</h3> <h4 id="crixet">Crixet</h4> <p>Home site: https://crixet.com Crixet delivers a cross-browser, cross-device experience with LaTeX-aware intelligent autocompletion and inline error detection. Its standout feature, Chirp, functions as an AI co-pilot: users can type, speak, or even sketch ideas to generate LaTeX code, graphs, or tables seamlessly. Crixet also integrates fast citation search‚Äîtightly coupled with Zotero‚Äîand supports one-click refactoring of code into modular files. A forthcoming on-premise edition promises full feature parity with the cloud service, enabling organizations to host Crixet within their own infrastructure for maximum compliance and data control‚Ä¶. #### TeXPage</p> <p>Home site: https://www.texpage.com TeXPage focuses on simplicity and security with end-to-end encrypted transmission and encrypted at-rest storage, along with multiple redundant backups. Its real-time collaboration supports synchronous editing, asynchronous workflows, and rich commenting with track-change visibility. Beyond standard LaTeX editing, TeXPage offers a visual formula editor, auto-generating tables via an intuitive GUI, and a dynamic file outline panel for quick navigation. Version history is fully transparent, allowing users to restore earlier drafts at any time. TeXPage‚Äôs commitment to encrypted workflows makes it especially appealing for institutions with strict data-privacy mandates.</p> <h4 id="inscriveio">inscrive.io</h4> <p>Home site: https://inscrive.io Positioning itself as the only fully GDPR-compliant cloud LaTeX editor, inscrive.io emphasizes performance and unmetered compilation. Collaborators enjoy real-time editing with unlimited participants, backed by extensive version control and AI-accelerated compile speeds. By guaranteeing EU-based data processing and compliance with the latest privacy standards, inscrive.io offers peace of mind to European universities and research consortia. Its streamlined interface prioritizes fast loading times and minimal configuration, so users can focus on content creation without infrastructure concerns.</p> <h3 id="ai-support">AI Support</h3> <p>Modern online LaTeX platforms are increasingly embedding AI-powered assistants to streamline writing, debugging, and formatting workflows. By leveraging these AI assistants, researchers can focus more on substantive content and less on low-level formatting, making LaTeX more accessible to novices and veterans alike. Below are several concrete implementations.</p> <p><strong>Writefull</strong> (an add-on for Overleaf) offers AI-powered language assistance tailored for academic writing.</p> <h4 id="intelligent-autocompletion-and-error-detection">Intelligent Autocompletion and Error Detection</h4> <p>Crixet‚Äôs ‚ÄúChirp‚Äù co-pilot (https://crixet.com/) uses AI to offer context-aware suggestions for macros, environments, and symbols, plus inline error detection that highlights compilation issues in real time.</p> <h4 id="natural-language-to-latex-conversion">Natural Language to LaTeX Conversion</h4> <p>Overleaf‚Äôs integration with Paperpal (https://paperpal.com/) lets users describe content in plain English‚Äîsuch as ‚Äúa quadratic formula‚Äù‚Äîand receive fully formatted LaTeX code in seconds.</p> <h4 id="voice-and-sketch-driven-content-creation">Voice and Sketch-Driven Content Creation</h4> <p>Crixet (https://crixet.com/) allows voice dictation or freehand sketches to generate LaTeX code and vector graphics, accelerating drafts for users less familiar with raw syntax.</p> <h4 id="grammar-and-style-coaching">Grammar and Style Coaching</h4> <p>Writefull AI (https://writefull.com/) integrates directly into Overleaf, providing academic-style grammar checks, passive-voice alerts, and rephrasing suggestions that respect LaTeX document structure.</p> <h4 id="citation-and-bibliography-automation">Citation and Bibliography Automation</h4> <p>TeXPage (https://www.texpage.com/) features AI-driven citation lookup with one-click DOI metadata retrieval, seamless Zotero integration, and automatic bibliography formatting for both numbered and author‚Äìyear styles.</p> <h4 id="collaborative-ai-driven-review">Collaborative AI-Driven Review</h4> <p>inscrive.io (https://inscrive.io/) employs AI to summarize recent edits, flag unresolved comments, and recommend missing citations or clarifications based on document content and version history.</p> <h2 id="feature-comparison">Feature Comparison</h2> <p>When selecting an online editor, you should consider your needs, requirements (such as security, privacy, data residency), budget constraints. The available solutions should be compared along the following dimensions:</p> <ol> <li> <p>Editing Capabilities and additional tools</p> </li> <li> <p>Collaboration</p> </li> <li> <p>Specific tools for academic writing (templates, journal submission automation)</p> </li> <li> <p>Simplicity</p> </li> <li> <p>Integration with external platform</p> </li> <li> <p>Cost and pricing plan available (including discounted offers for students or other categories)</p> </li> <li> <p>Resource limitations associated with the chosen plan such as compilation time, memory and storage limits</p> </li> </ol> <h3 id="editing-capabilities">Editing Capabilities</h3> <p>All major platforms provide syntax highlighting, autocompletion, and error detection, but differ in specialist features:</p> <ol> <li> <p>Overleaf: Most comprehensive mathematical palette</p> </li> <li> <p>Papeeria: In-editor plotting tools</p> </li> <li> <p>Authorea: Executable content (notebooks)</p> </li> </ol> <h3 id="templates">Templates</h3> <p>Overleaf: 400+ templates spanning papers, dissertations, CVs; Papeeria: Focused, specialist templates; Authorea: Journal-specific.</p> <h3 id="collaboration-all-offer-simultaneous-multi-user-editing-overleaf-has-conflict-resolution-and-version-history-papeeria-offers-discussion-features-authorea-emphasizes-data-sharing">Collaboration‚Ä¶ All offer simultaneous multi-user editing. Overleaf has conflict resolution and version history; Papeeria offers discussion features; Authorea emphasizes data-sharing.</h3> <h3 id="integration">Integration</h3> <p>Reference management (Zotero, Mendeley, Papers) and cloud sync (Dropbox, Google Drive, GitHub) vary by platform. The value of these features depend on your personal and team workflow.</p> <h2 id="pricing-comparison">Pricing Comparison</h2> <h3 id="overleaf-1">Overleaf</h3> <p>Free: Basic editing, 1 collaborator/project Standard ($199/year): 10 collaborators, history, advanced features Professional ($399/year): Unlimited collaborators, priority compile Institutional: Site-wide licenses with SSO, admin controls</p> <h3 id="papeeria-1">Papeeria</h3> <p>Delta plan: 5 USD /month, lower cost, most core features included</p> <p>Free premium for students</p> <h3 id="authorea-1">Authorea</h3> <p>Free for public docs; paid for private/research features</p> <h2 id="data-security-and-privacy">Data Security and Privacy</h2> <h3 id="gdpr-compliance">GDPR Compliance</h3> <p>EU institutions require platforms to meet GDPR for data processing, retention, deletion, and user control. Overleaf processes data in EU/US sites, is ISO/IEC 27001 certified, and meets data privacy framework requirements.</p> <h3 id="technical-safeguards">Technical Safeguards</h3> <p>All use HTTPS/TLS, encryption at rest, containerization, regular backups.</p> <h3 id="academic-risks">Academic Risks</h3> <p>Sensitive/unpublished content, patent material, or personal health data should be handled per institutional protocols. Platforms have had vulnerabilities in the past (e.g., ShareLaTeX command execution flaws), now mitigated with sandboxing and improved filtering.</p> <h3 id="security-best-practices">Security Best Practices</h3> <p>Institutions should:</p> <ul> <li> <p>Train users on sensitive content recognition</p> </li> <li> <p>Implement SSO/access controls</p> </li> <li> <p>Audit usage and backups</p> </li> <li> <p>Employ hybrid models for sensitive research</p> </li> </ul> <h2 id="comparative-recommendations">Comparative Recommendations</h2> <p>Based on the present survey and 5 years of daily user experience my favorite tools is <em>Overleaf</em>, but it is also the most expensive. I found a better features versus cost balance in <em>TeXPage</em> and <em>www.papeeria.com</em> 5 ‚Ç¨ per month plans. My general advises are:</p> <ul> <li> <p>If your research institution provide institutional account probably you should take advantage of this benefit.</p> </li> <li> <p>Create a free account, import a project representative of you typical paper a try to work on the project for at lest few hours. Some days is better if possible.</p> </li> <li> <p>If you are a <em>privacy-focused</em> user (or you have to work under security constrains) consider on-premises solutions or at least an hybrid solution with most sensitive data keep in private server while using a public cloud based collaboration platform. EU based platform and data center may also help with security and privacy requirements.</p> </li> </ul> <h2 id="future-trends">Future Trends</h2> <p>AI integration (grammar, citation management, formula generation) are rising, with privacy risks to consider. End-to-end encryption, decentralized models, and zero-trust architectures are on the horizon.</p> <h2 id="conclusion">Conclusion</h2> <p>Online LaTeX editors have radically improved academic writing workflows, with Overleaf leading in features and adoption. Data security and institutional policy remain central: hybrid approaches, regular audits, and robust training enable safe, productive research collaboration.</p>]]></content><author><name></name></author></entry><entry><title type="html">Eseguire un assistente AI basato su LLM in locale</title><link href="https://emanubuc.github.io/blog/2026/esecuzione-locale-llm/" rel="alternate" type="text/html" title="Eseguire un assistente AI basato su LLM in locale"/><published>2026-01-18T12:30:00+00:00</published><updated>2026-01-18T12:30:00+00:00</updated><id>https://emanubuc.github.io/blog/2026/esecuzione-locale-llm</id><content type="html" xml:base="https://emanubuc.github.io/blog/2026/esecuzione-locale-llm/"><![CDATA[<p>L‚Äôesecuzione di modelli di linguaggio di grandi dimensioni (LLM) in locale √® diventata oggi una realt√† pratica per utenti privati grazie alla maturazione di software intuitivi e a tecniche di compressione avanzate. Gestire un‚ÄôIA sul proprio PC o notebook offre vantaggi determinanti come la privacy assoluta dei dati, l‚Äôaccesso offline, l‚Äôassenza di costi di abbonamento e la possibilit√† di personalizzazione avanzata.</p> <p>Questa transizione √® stata resa possibile dalla convergenza di tre fattori critici: la disponibilit√† di modelli open-weight di classe enterprise, come le serie Llama, Mistral e Qwen; lo sviluppo di tecniche di compressione sofisticate note come quantizzazione; e la democratizzazione dell‚Äôhardware ad alte prestazioni, incluse le GPU consumer e i chip con memoria unificata.</p> <p>L‚Äôesecuzione locale di LLM √® diventata un‚Äôimperativo strategico per le organizzazioni che intendono mantenere la sovranit√† sui propri dati, ottimizzare i costi operativi e garantire la continuit√† del servizio in assenza di connettivit√† internet. Per supportare carichi di lavoro elevati in contesti aziendali (molti utenti e numero richieste di inferenza anche contemporanee) √® ncessario disporre di compenti hardware specifiche, dove la larghezza di banda della memoria e la VRAM della GPU risultano cruciali per garantire prestazioni accettabili. In questo articolo ci occupermo per√≤ solo dello scenario di utilizzo ‚Äúpersonale‚Äù (singolo utente) e per un utilizzo personale un notebook recente con 16/32GB di RAM o un Mac Mini possono esserre gi√† sufficienti.</p> <h2 id="1-installazione-ed-esecuzione-in-locale-di-un-llm">1. Installazione ed Esecuzione in Locale di un LLM</h2> <p>I modelli LLM non sono eseguibili autonomi ma vengono gestiti da un motore di inferenza. Tipicamente i modelli sono sviluppati con framework come TensorFlow, PyTorch o altri, che includono anche gli strumenti per il caricamento di un modello pre-addestrato e permettono di utilizzarlo per per operazioni di inferenze. Il motore di esecuzione gestisce l‚Äôallocazione della memoria, e le operazioni di calcolo distribuito su CPU o GPU. Per utilizzare un LLM (ma anche un altro qualsiasi altro modello di machine learning) in genere servono quattro elementi:</p> <ol> <li>Un modello (cio√® la struttura del modello)</li> <li>I ‚Äúpesi‚Äù generati dal processo di addestramento. In pratica si tratta dei valori numerici che alla fine del processo di addestramento sono stati assegnati ai parametri del modello. Un modello (architettura) dotato di 70B (Bilions= migliardi) di parametri prevede 70B di ‚Äúpesi‚Äù (valori da assegnare ai suoi parametri). Quando si <em>scarica</em> un modello in pratica si scarica il file con pesi (valori) da assegnare a tutti i parametri oltre che la struttura del modello con i parametri non valorizzati.</li> <li>Un motore di inferenza. A volte pu√≤ essere lo stesso usato per l‚Äôaddestramento, ma pu√≤ essere anche diverso e specializzato solo per l‚Äôinferenza come ad esempio Llama.cpp</li> <li>Hardware con risorse di calcolo sufficienti per il caricamento e l‚Äôesecuzione del modello</li> </ol> <p>Ad esempio √® possibile eseguire modelli pre-addestrati usanto da Libreria (<a href="https://huggingface.co/docs/transformers/model_doc/llama3">Transfromer </a>) di HugginFace in Python</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">transformers</span>
<span class="kn">import</span> <span class="n">torch</span>

<span class="n">model_id</span> <span class="o">=</span> <span class="sh">"</span><span class="s">meta-llama/Meta-Llama-3-8B</span><span class="sh">"</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">transformers</span><span class="p">.</span><span class="nf">pipeline</span><span class="p">(</span><span class="sh">"</span><span class="s">text-generation</span><span class="sh">"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_id</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">torch_dtype</span><span class="sh">"</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">},</span> <span class="n">device_map</span><span class="o">=</span><span class="sh">"</span><span class="s">auto</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">pipeline</span><span class="p">(</span><span class="sh">"</span><span class="s">Hey how are you doing today?</span><span class="sh">"</span><span class="p">)</span>

</code></pre></div></div> <p>Per un utilizzo al difuori dell‚Äôambiente di sviluppo √® solitamente preferibile utilizzare utilizzare dei motori di inferenza dedicati che permettono una maggiore efficienza nell‚Äôutilizzo delle risosrse di calcolo.</p> <p>I modelli (architettura e pesi) sono disponibili in vari formati. Negli ultimi anni si √® affermato il formato <a href="https://huggingface.co/docs/hub/gguf">GUFF</a> per la distribuzione di modelli complessi. La cosa importante √® che il formato sia compatibile con il modete di inferenza utilizzato!</p> <p>I motori di inferemza dispongono solitamente di una libreria di modelli gi√† preparata nel formato ottimale per lo specifico motore. La confersione √® possibile, ma si tratta di un processo che pu√≤ nascondere delle insidie e sicuramente da evitare durante i primi esperimenti con gli LLM.</p> <h2 id="2-requisiti-hardware">2. Requisiti Hardware</h2> <p>Sebbene gli LLM siano nati per girare su server potenti, l‚Äôhardware consumer attuale (2025/2026) pu√≤ gestire modelli di dimensioni contenute con prestazioni soddisfacenti.</p> <ul> <li><strong>Memoria RAM/VRAM:</strong> √à il fattore pi√π critico. Se il modello risiede interamente nella memoria video (<strong>VRAM</strong>) della scheda grafica (GPU), l‚Äôesecuzione √® da 5 a 20 volte pi√π rapida rispetto alla RAM di sistema. <ul> <li><strong>Configurazione Base (8GB RAM):</strong> Pu√≤ far girare piccoli modelli da 1B-3B parametri.</li> <li><strong>Configurazione Consigliata (16GB+ RAM):</strong> Il punto di equilibrio per modelli da <strong>7B-8B</strong> parametri, come Llama 3 o Mistral.</li> <li><strong>Apple Silicon (M1/M2/M3/M4):</strong> Questi chip offrono un vantaggio unico grazie alla <strong>Memoria Unificata</strong>, che permette alla GPU di accedere a tutta la RAM di sistema come se fosse VRAM.</li> </ul> </li> <li><strong>Scheda Video (GPU):</strong> Le schede <strong>NVIDIA RTX</strong> (serie 3000/4000/5000) sono lo standard grazie al supporto CUDA. Una GPU con <strong>8GB-12GB di VRAM</strong> (come la RTX 3060 o 4060) √® ideale per un‚Äôesperienza fluida con modelli medi.</li> <li><strong>Processore (CPU):</strong> Se non si dispone di una GPU dedicata, √® necessaria una CPU moderna (almeno 11¬™ gen Intel o AMD Zen 4) con supporto alle istruzioni <strong>AVX-512</strong> per accelerare i calcoli.</li> </ul> <h2 id="3-limportanza-della-quantizzazione">3. L‚Äôimportanza della Quantizzazione</h2> <p>Senza compressione, un modello da 7 miliardi di parametri richiederebbe circa 14-16 GB di VRAM, risultando inaccessibile per hardware standard. La <strong>quantizzazione</strong> riduce la precisione numerica dei pesi del modello (es. da 16-bit a 4-bit) senza comprometterne drasticamente l‚Äôintelligenza.</p> <p>Qualche esempio per capire l‚Äôeffetto della quantizzazione:</p> <ul> <li>FP32 precision: 4 byte per parameter. ==&gt; A 30B model = 120 GB just for weights (before KV cache) and without metadata!.</li> <li>FP16: 2 bytes/param ‚Üí 60 GB for 30B.</li> <li>Q8 (8-bit quant): ~1 byte/param ‚Üí 30 GB for 30B.</li> <li>Q4 (4-bit quant): ~0.5 bytes/param ‚Üí 15GB for 30B</li> </ul> <p>Oltre al file dei pesi, nel calcolo complessivo della memoria devono essere inclusi anche la cache KV ed i metadati che solitamente aggungono qualche altro GB al valore totale.</p> <p>Il formato <strong>Q4_K_M</strong> (4-bit) √® considerato il <strong>‚Äúgold standard‚Äù</strong> per l‚Äôuso locale: riduce le dimensioni del modello del 75%, permettendo a un modello 7B di girare su soli 5-6 GB di VRAM con una perdita di qualit√† trascurabile.</p> <p>Il tema dei formati dei modelli e della quantizzazione √® ampio e lo approfondiremo in un altra occasione.</p> <h2 id="4-strumenti-software-principali">4. Strumenti Software Principali</h2> <p>Esistono diverse piattaforme che riducono il processo di installazione e configuraizone a pochi click. La piattaforma di esecuzione pi√π performante per un utilizzo personale √® sicuramente <a href="https://github.com/ggml-org/llama.cpp">Llama.cpp</a>.</p> <p>Llama.cpp √® un‚Äôimplementazione in puro C/C++ creata da Georgi Gerganov. La sua importanza risiede nel fatto che √® stata la prima a dimostrare che i modelli linguistici di grandi dimensioni potevano girare in modo efficiente su hardware consumer (sia CPU che GPU) grazie a tecniche avanzate di quantizzazione e gestione della memoria. Supporta una vasta gamma di accelerazioni, tra cui CUDA (NVIDIA), Metal (Apple Silicon), ROCm (AMD) e Vulkan.</p> <p>Llama.cpp dispone di una sua interfaccia a riga di comando, di una interfaccia web in stile ChatGPT e di un server che ne permette l‚Äôutilizzo anche da parte di altri client.</p> <p>Ad esempio</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>llama-server -hf ggml-org/gpt-oss-20b-GGUF --n-cpu-moe 12 -c 32768 --jinja --no-mmap
</code></pre></div></div> <p>Avvia carica il modello GPT-OSS 20B ed avvia il server e l‚Äôinterfaccia web.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/llama-gpt-oss-480.webp 480w,/assets/img/llama-gpt-oss-800.webp 800w,/assets/img/llama-gpt-oss-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/llama-gpt-oss.png" width="100%" height="auto" title="Optional hover title" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1: llama-server in esecuzione.</figcaption> </figure> <p>Ovviamene il ‚Äúpeso‚Äù dell‚Äôesecuzione del modello si fa sentire e non permette di eseguire contemporanemante altre attivit√† pesanti per√≤ il sistema √® utilizzabile.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gpt-oss-web-interface-480.webp 480w,/assets/img/gpt-oss-web-interface-800.webp 800w,/assets/img/gpt-oss-web-interface-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gpt-oss-web-interface.png" width="100%" height="auto" title="Optional hover title" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 2: Interfaccia web in stile Chat esposta da llama-server.</figcaption> </figure> <p>Altri strumenti molto popolari, che aggiungono un interfaccia utente pi√π elaborata sopra Llama.cpp sacrificando per√≤ le prestazioni compessive: la differenza tra Llama.cpp ed altri strumenti come Ollama √® in alcuni casi √® piuttosto evidente con Llama.cpp che √® arriva ad essere anche un 30-40% pi√π veloce degli altri strumenti. Ad esempio, sul mio Mac Mini M2 16GB con Llama.cpp il modello GPT-OSS 20B √® utilizzabile, mentre con altri strumenti riesce a malapena ad avviarsi e poi √® troppo lento per un qualsiasi utilizzo pratico interattivo.</p> <p>Altri strumenti popolari per eseguire LLM in locale sono:</p> <ul> <li><strong>LM Studio:</strong> Forse la soluzione pi√π accessibile per utenti non tecnici. Offre un‚Äôinterfaccia grafica completa, un motore di ricerca integrato per scaricare modelli da Hugging Face e permette di visualizzare l‚Äôuso delle risorse in tempo reale.</li> <li><strong>Ollama:</strong> Estremamente popolare tra gli utenti pi√π esperti e gli sviluppatori. Funziona tramite riga di comando (CLI) ma pu√≤ essere abbinato a interfacce web grafiche come <strong>OpenWebUI</strong> per un‚Äôesperienza simile a ChatGPT.</li> <li><strong>Jan.ai:</strong> Un‚Äôalternativa <strong>open-source</strong> focalizzata sulla privacy che funziona interamente offline. Ha un‚Äôinterfaccia minimalista e supporta l‚Äôestensione tramite plugin.</li> <li><strong>GPT4All:</strong> Ottimizzato per girare su hardware modesto (anche solo CPU). Include la funzione <strong>LocalDocs</strong>, che permette di ‚Äúchattare‚Äù con i propri documenti locali (PDF, Word) in modo semplice.</li> </ul> <h2 id="5-modelli-consigliati-per-hardware-standard">5. Modelli Consigliati per Hardware Standard</h2> <p>Per un PC o notebook standard, suggerisco di iniziare con le versioni pi√π piccole e quantizzate di queste famiglie di modelli:</p> <ul> <li><strong>Llama 3.2 (1B / 3B):</strong> Ultra-leggeri, ideali per notebook con poca RAM.</li> <li><strong>Mistral (7B) / Llama 3.1 (8B):</strong> I modelli pi√π versatili per compiti generali e scrittura.</li> <li><strong>Qwen 2.5/3 (0.5B - 7B):</strong> Eccellenti per il coding e compiti multilingue.</li> <li><strong>Phi-3/4 (Mini):</strong> Modelli piccoli di Microsoft con capacit√† di ragionamento sorprendenti per la loro dimensione.</li> </ul> <p>Una volta testato il funzionamento di un modello di piccole dimensioni, vi consiglio di testare anche modelli pi√π grandi fino a trovare il compromesso ottimale tra utilizzo delle risorse di calcolo disponibili e prestazioni.</p> <h3 id="openai-gpt-oss">OpenAI GPT-OSS</h3> <p>OpenAI GPT-OSS (Agosto 2025) √® il primo rilascio open-weight significativo di OpenAI dopo anni. Il modello √® disponibile principalmente nelle versioni 20B e 120B. La variante 120B, in particolare, offre prestazioni di livello GPT-4 ed eccelle in compiti di ragionamento avanzato, tool calling e workflow agentici complessi.</p> <ul> <li> <p>La versione 20B √® quella pi√π indicata per hardware locale avanzato, ma richiede comunque una configurazione di fascia alta con almeno 32GB di RAM/VRAM per girare correttamente. <em>Sul mio Mac Mini Apple Silicon M2 con 16GB di RAM condivisa</em> si riesce ad eseguire usando LLAMA-CPP <a href="https://github.com/ggml-org/llama.cpp/discussions/15396">seguendo la guida dedicata a GPT-OSS</a></p> </li> <li> <p>La versione 120B rimane fuori dalla portata dei PC standard, richiedendo solitamente infrastrutture di classe enterprise.</p> </li> </ul> <p><strong>In sintesi:</strong> Per iniziare √® sufficiente un notebook moderno con <strong>16GB di RAM</strong> (o acora meglio un Mackbook o un MacMini), scaricare <strong>LM Studio</strong> e provare un modello come <strong>Mistral</strong> o <strong>Llama 3.1 8B</strong> in versione quantizzata a 4-bit. Se la memoria video non √® sufficiente, questi strumenti sposteranno automaticamente parte del lavoro sulla RAM di sistema, sacrificando la velocit√† per garantire l‚Äôesecuzione.</p> <p>Con un Mac Mini, equipaggiato con processore AppleSilicon serie M, si ottiene un rapporto prezzo/prestazioni sicuramente interessante e la possibilit√† di eseguire modelli di dimensione fino a 20B.</p>]]></content><author><name></name></author><category term="AI"/><category term="LLM"/><category term="AI"/><summary type="html"><![CDATA[L'esecuzione di modelli di linguaggio di grandi dimensioni (LLM) in locale √® diventata oggi una realt√† pratica per utenti privati grazie alla maturazione di software intuitivi e a tecniche di compressione avanzate]]></summary></entry><entry><title type="html">Creare un libro in famiglia. Molto pi√π di un gioco</title><link href="https://emanubuc.github.io/blog/2026/progetto-creare-un-libro/" rel="alternate" type="text/html" title="Creare un libro in famiglia. Molto pi√π di un gioco"/><published>2026-01-01T12:30:00+00:00</published><updated>2026-01-01T12:30:00+00:00</updated><id>https://emanubuc.github.io/blog/2026/progetto-creare-un-libro</id><content type="html" xml:base="https://emanubuc.github.io/blog/2026/progetto-creare-un-libro/"><![CDATA[<h1 id="-creare-un-libro-in-famiglia-molto-pi√π-di-un-gioco">üìö Creare un libro in famiglia: molto pi√π di un gioco</h1> <p>Avete mai pensato di trasformare i racconti della buonanotte o i disegni sparsi sul tavolo in un vero libro? Recentemente abbiamo avviato un laboratorio creativo familiare che si √® concluso con una prima pubblicazione reale, e altri progetti sono ancora in corso.</p> <p>Abbiamo affrontato un vero e proprio progetto, suddiviso in sei fasi chiave, in cui ogni membro della famiglia ha avuto un ruolo fondamentale.</p> <h2 id="-le-6-fasi-del-processo">üõ† Le 6 fasi del processo</h2> <p>Per arrivare al risultato finale, abbiamo seguito un percorso strutturato:</p> <ol> <li><strong>Ideazione:</strong> Il brainstorming collettivo per decidere il tema e i personaggi.</li> <li><strong>Creazione testi:</strong> Mettere nero su bianco la storia, curando il ritmo e il linguaggio.</li> <li><strong>Creazione illustrazioni:</strong> La fase pi√π libera, dove la creativit√† visiva ha preso forma.</li> <li><strong>Impaginazione e composizione:</strong> Qui √® entrata in gioco la <strong>tecnologia</strong>. Abbiamo usato software di impaginazione, elaborazione immagini e scrittura per realizzare il master digitale del libro. I disegni sono stati realizzati a mano: niente AI generativa per questo progetto ‚Äî carta, pastelli e scanner.</li> <li><strong>Revisione delle bozze:</strong> Un esercizio di attenzione e lettura critica per individuare refusi, errori di impaginazione e valutare possibili alternative.</li> <li><strong>Distribuzione e pubblicazione:</strong> Scegliere canale e modalit√† di pubblicazione e distribuzione si √® rivelato pi√π impegnativo del previsto. Le opzioni sono numerose e abbiamo imparato molto.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/processo_creazione_libro_con_bambini-480.webp 480w,/assets/img/processo_creazione_libro_con_bambini-800.webp 800w,/assets/img/processo_creazione_libro_con_bambini-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/processo_creazione_libro_con_bambini.png" width="100%" height="auto" title="Optional hover title" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1: Le sei fasi principali del processo di creazione che abbiamo seguito.</figcaption> </figure> <h2 id="-competenze-e-decisioni-strategiche">üí° Competenze e Decisioni Strategiche</h2> <p>Realizzare un libro richiede un mix di competenze <strong>analogiche e digitali</strong>: scrittura, lettura, disegno a mano libera e grafica digitale. Ma la lezione pi√π grande √® stata imparare il <strong>lavoro di squadra</strong> su un progetto a lungo termine con ruoli e responsabilit√† ben definiti. Si tratta di un‚Äôesperienza comune per gli adulti in molti contesti lavorativi, ma per i bambini della scuola dell‚Äôinfanzia e della primaria √® qualcosa di nuovo.</p> <p>Ogni opera √® un progetto a s√© stante che richiede scelte strategiche che poi orientano tutto il flusso di lavoro. Ad esempio, per il nostro libro <strong>‚ÄúI Gatti e le Stagioni‚Äù</strong> abbiamo deciso di escludere gli aspetti commerciali dal progetto: l‚Äôinvestimento economico √® stato ridotto al minimo, limitato alle bozze stampate in tipografia, e il libro viene distribuito al prezzo minimo possibile, senza alcun guadagno per noi.</p> <p>Questa scelta ha influenzato tutto il processo, dalla selezione del formato alle scelte sui canali di distribuzione, insegnando ai bambini che ogni progetto nasce da un obiettivo chiaro.</p> <h2 id="-il-valore-del-risultato">üèÜ Il valore del risultato</h2> <p>Vedere il proprio nome su una copertina e il libro disponibile per l‚Äôacquisto accanto agli altri √® un <strong>successo tangibile</strong>. Dimostra ai bambini che con l‚Äôimpegno, l‚Äôorganizzazione e la collaborazione si possono raggiungere risultati importanti che restano nel tempo.</p> <p>√à una scelta di grande valore simbolico ed educativo. Se il digitale offre accessibilit√† e velocit√†, il <strong>libro cartaceo</strong> trasforma l‚Äôesperienza creativa in un <strong>oggetto di memoria</strong>.</p> <h2 id="-perch√©-abbiamo-scelto-la-carta">üìñ Perch√© abbiamo scelto la carta</h2> <p>In un‚Äôepoca in cui tutto scorre velocemente su uno schermo, abbiamo deciso di <strong>stampare il nostro libro.</strong> Oltre alla versione e-book, ‚ÄúI Gatti e le Stagioni‚Äù √® diventato un oggetto fisico, solido, da sfogliare.</p> <p>Noi amiamo molto i libri e leggiamo tutti i giorni insieme ai bambini. I bambini sono abituati a maneggiare i loro libri e vedere libri in mano a noi genitori. Per un bambino di 5 anni, il concetto di ‚Äúfile digitale‚Äù √® astratto. Tenere in mano il volume, sentire l‚Äôodore della carta e il peso delle pagine d√† una consistenza reale alla loro fatica. Non √® pi√π ‚Äúun disegno nel computer del pap√†‚Äù, ma <strong>un‚Äôopera vera</strong> che occupa uno spazio fisico nella libreria di casa.</p> <p>C‚Äô√® anche un altro aspetto: il digitale √® fragile: i formati cambiano, gli account si perdono, i dispositivi si rompono. Un libro fisico √® una <strong>promessa di durata</strong>. Abbiamo spiegato ai bambini che questo libro √® un pezzetto della loro infanzia. Un giorno, potranno mostrarlo ai loro figli, dicendo: <em>‚ÄúGuarda, questo l‚Äôho disegnato io quando avevo la tua et√†‚Äù</em>.</p> <p>C‚Äô√® anche un altro aspetto: il digitale √® fragile: i formati cambiano, gli account si perdono, i dispositivi si rompono. Un libro fisico √® una <strong>promessa di durata</strong>. Abbiamo spiegato ai bambini che questo libro √® un pezzetto della loro infanzia. Un giorno, potranno mostrarlo ai loro figli, dicendo: <em>‚ÄúGuarda, questo l‚Äôho disegnato io quando avevo la tua et√†‚Äù</em>.</p>]]></content><author><name></name></author><category term="writing"/><category term="writing"/><category term="content"/><category term="creation"/><category term="project"/><summary type="html"><![CDATA[Avete mai pensato di trasformare i racconti della buonanotte o i disegni sparsi sul tavolo in un vero libro? Recentemente abbiamo organizzato un laboratorio creativo familiare che si √® concluso con una pubblicazione reale.]]></summary></entry><entry><title type="html">Dove pubblicare i tuoi tutorial e dispense di informatica</title><link href="https://emanubuc.github.io/blog/2025/Dove_pubblicare_tutorial_e_dispense_informatica/" rel="alternate" type="text/html" title="Dove pubblicare i tuoi tutorial e dispense di informatica"/><published>2025-10-03T11:46:00+00:00</published><updated>2025-10-03T11:46:00+00:00</updated><id>https://emanubuc.github.io/blog/2025/Dove_pubblicare_tutorial_e_dispense_informatica</id><content type="html" xml:base="https://emanubuc.github.io/blog/2025/Dove_pubblicare_tutorial_e_dispense_informatica/"><![CDATA[<h1 id="dove-pubblicare-i-tuoi-tutorial-e-dispense-di-informatica-quattro-piattaforme-per-citabilit√†-e-riconoscimento">Dove pubblicare i tuoi tutorial e dispense di informatica: quattro piattaforme per citabilit√† e riconoscimento</h1> <p>Hai scritto delle <strong>dispense</strong> dettagliate su machine learning, un <strong>tutorial</strong> su Python o una <strong>guida tecnica</strong> su Kubernetes? Ottimo!</p> <p>Prima di limitarti a un PDF sul tuo sito personale, a un post sul tuo blog o a un articolo su Medium, sappi che esistono piattaforme pensate per il mondo accademico e tecnico che ti permettono non solo di raggiungere una community mirata, ma anche di rendere il tuo lavoro permanentemente citabile da altri ricercatori e professionisti.</p> <p>Oltre a scrivere un post su Medium, √® utile pubblicare il tuo lavoro su una piattaforma che garantisca la <strong>citabilit√† permanente</strong> (tramite DOI o ID persistente) e l‚Äô<strong>indicizzazione</strong> nei motori di ricerca accademici.</p> <p>Se pensi di pubblicare lo stesso materiale (o anche solo una parte di esso) su una rivista accademica, verifica prima la politica dell‚Äôeditore (o della specifica rivista) sui cosiddetti <em>preprint</em>. Molte riviste accettano articoli che sono stati precedentemente pubblicati su preprint server riconosciuti, ma √® sempre meglio controllare le regole specifiche della rivista.</p> <p>Di seguito trovi quattro ottime opzioni, classificate in base al loro focus disciplinare e alla tipologia di materiale che accettano.</p> <hr/> <h2 id="1-techrxiv-lautorit√†-tecnica-per-ingegneria-e-informatica-ieee">1. TechRxiv: l‚Äôautorit√† tecnica per ingegneria e informatica (IEEE)</h2> <p>Se le tue dispense riguardano il machine learning, la robotica o l‚Äôingegneria elettrica, <strong>TechRxiv</strong> √® una scelta mirata.</p> <ul> <li><strong>Cos‚Äô√®:</strong> un server di preprint moderato, <strong>supportato dall‚ÄôIEEE</strong> (Institute of Electrical and Electronics Engineers).</li> <li><strong>Visibilit√† &amp; indicizzazione:</strong> eccellente ‚Äî √® riconosciuto e indicizzato da <strong>Google Scholar</strong>, quindi il tuo lavoro sar√† facilmente reperibile dalla comunit√† accademica di settore.</li> <li><strong>Citabilit√†:</strong> assegna un <strong>identificatore persistente</strong> che permette la citazione formale.</li> <li><strong>Ideale per:</strong> dispense di ML, report tecnici, guide avanzate su hardware o protocolli.</li> </ul> <hr/> <h2 id="2-arxiv-la-vetrina-della-ricerca-pura-massima-diffusione">2. arXiv: la vetrina della ricerca pura (massima diffusione)</h2> <p>Se le tue dispense sono strutturate come un <strong>survey</strong> o un <strong>report tecnico</strong> con l‚Äôobiettivo di raggiungere i ricercatori, prendi in considerazione <strong>arXiv</strong>.</p> <ul> <li><strong>Cos‚Äô√®:</strong> il server di preprint originale e pi√π grande, ampiamente usato nelle comunit√† di fisica, matematica e computer science (es. <code class="language-plaintext highlighter-rouge">cs.LG</code> per machine learning).</li> <li><strong>Visibilit√† &amp; indicizzazione:</strong> massima ‚Äî √® lo standard de facto per la diffusione rapida della ricerca e gode di eccellente indicizzazione su Google Scholar.</li> <li><strong>Citabilit√†:</strong> utilizza un identificatore unico (<code class="language-plaintext highlighter-rouge">arXiv:YYMM.#####</code>) universalmente riconosciuto.</li> <li><strong>Formato:</strong> il processo di submission richiede solitamente i sorgenti LaTeX.</li> <li><strong>Attenzione:</strong> arXiv √® orientato alla ricerca originale; report e survey sono accettati se i moderatori li ritengono scientificamente rilevanti.</li> </ul> <hr/> <h2 id="3-zenodo-larchivio-flessibile-doi-e-archiviazione-multi-formato">3. Zenodo: l‚Äôarchivio flessibile (DOI e archiviazione multi-formato)</h2> <p>Se cerchi la soluzione pi√π semplice che garantisca un <strong>DOI</strong> e permetta di archiviare vari materiali, <strong>Zenodo</strong> √® molto indicato.</p> <ul> <li><strong>Cos‚Äô√®:</strong> un repository ad accesso aperto gestito dal <strong>CERN</strong>; accetta qualsiasi output della ricerca, inclusi report, software, dataset e materiale didattico.</li> <li><strong>Visibilit√† &amp; indicizzazione:</strong> buona ‚Äî indicizzato da Google Search e Google Dataset Search; l‚Äôindicizzazione in Google Scholar non √® automatica come per arXiv o TechRxiv, ma il DOI garantisce la citabilit√†.</li> <li><strong>Citabilit√†:</strong> assegna un <strong>DOI (Digital Object Identifier)</strong> a ogni pubblicazione.</li> <li><strong>Ideale per:</strong> dispense accompagnate da codice sorgente o dataset (tutto in un unico record DOI).</li> </ul> <hr/> <h2 id="4-ssrn-la-rete-di-lavoro-accademico-focus-su-working-paper">4. SSRN: la rete di lavoro accademico (focus su working paper)</h2> <p>SSRN √® un‚Äôopzione valida soprattutto se il tuo lavoro si colloca all‚Äôintersezione tra informatica e altre discipline come economia o diritto.</p> <ul> <li><strong>Cos‚Äô√®:</strong> nato per le scienze sociali, oggi di propriet√† di Elsevier, si √® ampliato includendo anche ambiti di computer science e engineering; √® focalizzato sui <strong>working paper</strong>.</li> <li><strong>Visibilit√† &amp; indicizzazione:</strong> molto buona ‚Äî particolarmente diffuso negli Stati Uniti per la diffusione preliminare della ricerca.</li> <li><strong>Citabilit√†:</strong> fornisce un <strong>identificatore persistente</strong> ed √® integrato con varie metriche accademiche.</li> <li><strong>Ideale per:</strong> documenti in stile ‚Äúreport accademico‚Äù o working paper, o lavori con forte intersezione con scienze sociali.</li> </ul> <hr/> <h3 id="quale-scegliere">Quale scegliere?</h3> <p>La scelta dipende dall‚Äôargomento, dallo scopo della pubblicazione e dal tipo di materiale (documento LaTeX, documento Word, file con dataset, codice sorgente, software associato, ecc.). Esempio per dispense di machine learning gi√† pronte in LaTeX/PDF:</p> <table> <thead> <tr> <th style="text-align: left">Scenario</th> <th style="text-align: left">Piattaforma raccomandata</th> <th style="text-align: left">Motivo principale</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Priorit√†: visibilit√† accademica e reputazione settoriale</strong></td> <td style="text-align: left"><strong>TechRxiv</strong></td> <td style="text-align: left">Supportato dall‚ÄôIEEE, ottima indicizzazione in Google Scholar per il settore.</td> </tr> <tr> <td style="text-align: left"><strong>Priorit√†: flessibilit√†, DOI standard, archivio dati/codice</strong></td> <td style="text-align: left"><strong>Zenodo</strong></td> <td style="text-align: left">Semplice da usare e garantisce un DOI per ogni tipo di output.</td> </tr> <tr> <td style="text-align: left"><strong>Priorit√†: massima diffusione nella ricerca pura</strong></td> <td style="text-align: left"><strong>arXiv</strong></td> <td style="text-align: left">Piattaforma leader per i preprint in computer science.</td> </tr> </tbody> </table> <p>Scegliendo una di queste piattaforme, ti assicuri che le tue dispense non siano solo lette, ma diventino parte della letteratura citabile della comunit√†.</p>]]></content><author><name></name></author><category term="research"/><category term="preprint"/><category term="openaccess"/><summary type="html"><![CDATA[piattaforma online dove pubblicare tutorial, dispense e risultati di ricerca]]></summary></entry><entry><title type="html">Hosting a Personal Website on GitHub Pages</title><link href="https://emanubuc.github.io/blog/2025/hosting-website-github/" rel="alternate" type="text/html" title="Hosting a Personal Website on GitHub Pages"/><published>2025-03-26T14:24:00+00:00</published><updated>2025-03-26T14:24:00+00:00</updated><id>https://emanubuc.github.io/blog/2025/hosting-website-github</id><content type="html" xml:base="https://emanubuc.github.io/blog/2025/hosting-website-github/"><![CDATA[<p>I recently decided it was time to refresh my personal website. Since I prefer to avoid dealing with virtual servers and hosting maintenance, GitHub Pages has become my go-to solution.</p> <p>Hosting a personal website on GitHub is a powerful way to establish a professional online presence with minimal cost and maintenance. In this post, I‚Äôll walk you through the key concepts, benefits, and a practical workflow for getting started with GitHub Pages and a simple static site.</p> <h2 id="why-use-github-for-your-personal-website">Why Use GitHub for Your Personal Website</h2> <p>GitHub offers a feature called GitHub Pages, which can host static websites directly from a repository. This is ideal for personal portfolios, research pages, and blogs written in Markdown.</p> <p><strong>Key advantages include:</strong></p> <ul> <li><strong>Free</strong> hosting with automatic HTTPS and no bandwidth limits for typical personal use.</li> <li>Tight integration with Git: your site is version-controlled, reviewable, and easy to roll back.</li> <li>Native support for static site generators like Jekyll, making Markdown-based blogs and documentation straightforward to manage.</li> </ul> <h2 id="understanding-github-pages">Understanding GitHub Pages</h2> <p>GitHub Pages serves HTML, CSS, JavaScript, and other static assets from a special branch or repository. For a user site, the repository is typically named <code class="language-plaintext highlighter-rouge">username.github.io</code>, and GitHub automatically builds and publishes it.</p> <p><strong>Important characteristics:</strong></p> <ul> <li>Content is static: no server-side code such as Python, PHP, or Node.js runs on GitHub Pages.</li> <li>Sites can be either user/organization sites (<code class="language-plaintext highlighter-rouge">username.github.io</code>) or project-specific sites under any repository.</li> <li>Configuration is managed via repository settings and simple configuration files, so most changes are handled through Git commits.</li> </ul> <h2 id="choosing-a-site-structure-and-workflow">Choosing a Site Structure and Workflow</h2> <p>Before writing content, decide how you want to manage your site:</p> <ul> <li>For a minimal portfolio, a hand-written HTML page plus a stylesheet may be enough.</li> <li>For content-heavy sites or blogs, using a static site generator like Jekyll simplifies layout reuse, navigation, and Markdown-based authoring.</li> <li>A hybrid approach often works well: a Jekyll site with separate sections for an ‚ÄúAbout‚Äù page, a research or projects portfolio, and a blog.</li> <li>A template-based website can offer more customization options.</li> </ul> <h3 id="1-simple-htmlcssjavascript">1. Simple HTML/CSS/JavaScript</h3> <ul> <li>Create a repository named <code class="language-plaintext highlighter-rouge">yourusername.github.io</code></li> <li>Add <code class="language-plaintext highlighter-rouge">index.html</code>, CSS, and JS files</li> <li>Enable GitHub Pages in Settings</li> <li>Your site goes live at <code class="language-plaintext highlighter-rouge">https://yourusername.github.io</code></li> </ul> <h3 id="2-using-jekyll-githubs-built-in-generator">2. Using Jekyll (GitHub‚Äôs Built-in Generator)</h3> <ul> <li>Jekyll is automatically supported by GitHub Pages</li> <li>Supports Markdown-based blog posts and pages</li> <li>Choose from GitHub‚Äôs supported themes (Minima, Hacker, etc.)</li> <li>Ideal if you want blogging capabilities</li> </ul> <h3 id="3-using-pre-built-templates">3. Using Pre-built Templates</h3> <ul> <li>Clone a portfolio template repository (like <code class="language-plaintext highlighter-rouge">al-folio</code> or others from GitHub Collections)</li> <li>Customize with your content</li> <li>Deploy with a single push</li> </ul> <h3 id="key-features-across-approaches">Key Features Across Approaches</h3> <ul> <li>‚úÖ Free hosting</li> <li>‚úÖ Free HTTPS</li> <li>‚úÖ Custom domain support (optional)</li> <li>‚úÖ No backend server needed (static sites only)</li> <li>‚úÖ Built-in version control</li> <li>‚úÖ Easy updates via Git</li> </ul> <hr/>]]></content><author><name></name></author><category term="hosting"/><category term="hosting"/><category term="website"/><category term="github"/><summary type="html"><![CDATA[I recently decided to refresh my personal website. Since I prefer to avoid server and hosting maintenance, GitHub Pages is my go-to solution.]]></summary></entry><entry><title type="html">Differential Privacy</title><link href="https://emanubuc.github.io/blog/2021/Differential_Privacy/" rel="alternate" type="text/html" title="Differential Privacy"/><published>2021-04-09T10:05:00+00:00</published><updated>2021-04-09T10:05:00+00:00</updated><id>https://emanubuc.github.io/blog/2021/Differential_Privacy</id><content type="html" xml:base="https://emanubuc.github.io/blog/2021/Differential_Privacy/"><![CDATA[<h1 id="differential-privacy">Differential Privacy</h1> <p>Data scientists have an ethical (and often legal) responsibility to protect sensitive data. Differential privacy is a leading edge approach that enables useful analysis while protecting individually identifiable data values.</p> <p>A machine learning project typically involves an iterative process of data analyses in order to gain insights into the data and determine which variables are most likely to help build predictive models. Analyzing data usually involves aggregative and statistical functions that provide insights into the statistical distribution of variables and the relationships between them. With large volumes of data, the aggregations provide a level of abstraction; but with smaller amounts of data, or with repeated analyses, even aggregated results may reveal details about individual observations.</p> <p>Differential privacy is a technique that is designed to preserve the privacy of individual data points by adding ‚Äúnoise‚Äù to the data. The goal is to ensure that enough noise is added to provide privacy for individual values while ensuring that the overall statistical makeup of the data remains consistent, and aggregations produce statistically similar results as when used with the original raw data.</p> <h2 id="smartnoisesdk">SmartNoiseSDK</h2> <p><a href="https://smartnoise.org/">SmartNoise</a> is a toolkit from OpenDP; a joint project between researchers at Microsoft, Harvard University, and other contributors that aims to provide building blocks for using differential privacy in data analysis and machine learning projects.</p> <p><img src="/assets/img/smart-noise-sdk-main-graphic.png" alt="smart noise SDK graphic"/></p> <h3 id="how-to-install">How To Install</h3> <pre><code class="language-Python">!pip install opendp-smartnoise==0.1.3.1
</code></pre> <h3 id="add-statistical-noise">Add Statistical Noise</h3> <p>You can use SmartNoise to create an analysis in which noise is added to the source data. The underlying mathematics of how the noise is added can be quite complex, but SmartNoise takes care of most of the details for you. However, there are a few concepts it‚Äôs useful to be aware of.</p> <p><strong>Upper and lower bounds</strong>: Clamping is used to set upper and lower bounds on values for a variable. This is required to ensure that the noise generated by SmartNoise is consistent with the expected distribution of the original data. <strong>Sample size</strong>: To generate consistent differentially private data for some aggregations, SmartNoise needs to know the size of the data sample to be generated. <strong>Epsilon</strong>: Put simplistically, epsilon is a non-negative value that provides an inverse measure of the amount of noise added to the data. A low epsilon results in a dataset with a greater level of privacy, while a high epsilon results in a dataset that is closer to the original data. Generally, you should use epsilon values between 0 and 1. Epsilon is correlated with another value named delta, that indicates the probability that a report generated by an analysis is not fully private.</p> <p>As a rule of thumb, $\epsilon$ should be thought of as a small number, between approximately 1/1000and 1. In each implementation of differential privacy, a value of $\epsilon$ that allows a reasonable compromise between privacy and accuracy should be carefully chosen.</p> <p>see https://privacytools.seas.harvard.edu/files/privacytools/files/pedagogical-document-dp_0.pdf</p>]]></content><author><name></name></author><category term="research"/><category term="privacy"/><category term="ML"/><category term="privacy"/><category term="ML"/><summary type="html"><![CDATA[Differential privacy is a leading edge approach that enables useful analysis while protecting individually identifiable data values]]></summary></entry><entry><title type="html">Azure Machine Learning experiments</title><link href="https://emanubuc.github.io/blog/2021/AzureML_Exeperiments/" rel="alternate" type="text/html" title="Azure Machine Learning experiments"/><published>2021-03-11T10:05:00+00:00</published><updated>2021-03-11T10:05:00+00:00</updated><id>https://emanubuc.github.io/blog/2021/AzureML_Exeperiments</id><content type="html" xml:base="https://emanubuc.github.io/blog/2021/AzureML_Exeperiments/"><![CDATA[<h1 id="azure-machine-learning-experiments">Azure Machine Learning experiments</h1> <p>Like any scientific discipline, data science involves running experiments; typically to explore data or to build and evaluate predictive models. In Azure Machine Learning, an experiment is a named process, usually the running of a script or a pipeline, that can generate metrics and outputs and be tracked in the Azure Machine Learning workspace.</p> <p><img src="/assets/img/azure_ml_03-01-experiment.jpg" alt="AzureML Experiment"/></p> <p>An experiment can be run multiple times, with different data, code, or settings; and Azure Machine Learning tracks each run, enabling you to view run history and compare results for each run.</p> <h2 id="the-experiment-run-context">The Experiment Run Context</h2> <p>When you submit an experiment, you use its run context to initialize and end the experiment run that is tracked in Azure Machine Learning, as shown in the following code sample:</p> <pre><code class="language-Python">from azureml.core import Experiment

# create an experiment variable
experiment = Experiment(workspace = ws, name = "my-experiment")

# start the experiment
run = experiment.start_logging()

# experiment code goes here

# end the experiment
run.complete()
</code></pre> <h2 id="logging-metrics-and-creating-outputs">Logging Metrics and Creating Outputs</h2> <p>Experiments are most useful when they produce metrics and outputs that can be tracked across runs</p> <h3 id="logging-metrics">Logging Metrics</h3> <p>Every experiment generates log files that include the messages that would be written to the terminal during interactive execution. This enables you to use simple print statements to write messages to the log. However, if you want to record named metrics for comparison across runs, you can do so by using the Run object; which provides a range of logging functions specifically for this purpose. These include:</p> <ul> <li>log: Record a single named value.</li> <li>log_list: Record a named list of values.</li> <li>log_row: Record a row with multiple columns.</li> <li>log_table: Record a dictionary as a table.</li> <li>log_image: Record an image file or a plot.</li> </ul> <p>For example, following code records the number of observations (records) in a CSV file:</p> <pre><code class="language-Python">from azureml.core import Experiment
import pandas as pd

# Create an Azure ML experiment in your workspace
experiment = Experiment(workspace = ws, name = 'my-experiment')

# Start logging data from the experiment
run = experiment.start_logging()

# load the dataset and count the rows
data = pd.read_csv('data.csv')
row_count = (len(data))

# Log the row count
run.log('observations', row_count)

# Complete the experiment
run.complete()

</code></pre>]]></content><author><name></name></author><category term="research"/><category term="Azure"/><category term="AI"/><category term="ML"/><category term="Azure"/><category term="ML"/><category term="MLOPs"/><summary type="html"><![CDATA[In Azure Machine Learning, an experiment is a named process, usually the running of a script or a pipeline, that can generate metrics and outputs and be tracked in the Azure Machine Learning workspace]]></summary></entry><entry><title type="html">Azure Machine Learning</title><link href="https://emanubuc.github.io/blog/2021/Azure_Machine_Learning/" rel="alternate" type="text/html" title="Azure Machine Learning"/><published>2021-03-10T11:15:00+00:00</published><updated>2021-03-10T11:15:00+00:00</updated><id>https://emanubuc.github.io/blog/2021/Azure_Machine_Learning</id><content type="html" xml:base="https://emanubuc.github.io/blog/2021/Azure_Machine_Learning/"><![CDATA[<h1 id="azure-machine-learning">Azure Machine Learning</h1> <p>Azure Machine Learning is a cloud-based platform for building and operating machine learning solutions in Azure. It includes a wide range of features and capabilities that help data scientists prepare data, train models, publish predictive services, and monitor their usage. Most importantly, it helps data scientists increase their efficiency by automating many of the time-consuming tasks associated with training models; and it enables them to use cloud-based compute resources that scale effectively to handle large volumes of data while incurring costs only when actually used.</p> <p><img src="/assets/img/01-01-what-is-azure-ml.jpg" alt="azure ml"/></p> <p>Built on the Microsoft Azure cloud platform, Azure Machine Learning enables you to manage:</p> <p>Scalable on-demand compute for machine learning workloads. Data storage and connectivity to ingest data from a wide range sources. Machine learning workflow orchestration to automate model training, deployment, and management processes. Model registration and management, so you can track multiple versions of models and the data on which they were trained. Metrics and monitoring for training experiments, datasets, and published services. Model deployment for real-time and batch inferencing.</p> <h2 id="azure-machine-learning-workspace">Azure MAchine Learning Workspace</h2> <p>To use Azure Machine Learning, you create a workspace in your Azure subscription. You can then use this workspace to manage data, compute resources, code, models, and other artifacts related to your machine learning workloads.</p> <p>A workspace is a context for the experiments, data, compute targets, and other assets associated with a machine learning workload.</p> <p>The assets in a workspace include:</p> <ul> <li>Compute targets for development, training, and deployment.</li> <li>Data for experimentation and model training.</li> <li>Notebooks containing shared code and documentation.</li> <li>Experiments, including run history with logged metrics and outputs.</li> <li>Pipelines that define orchestrated multi-step processes.</li> <li>Models that you have trained.</li> </ul> <h3 id="workspaces-as-azure-resources">Workspaces as Azure Resources</h3> <p>Workspaces are Azure resources, and as such they are defined within a resource group in an Azure subscription, along with other related Azure resources that are required to support the workspace.</p> <p><img src="/assets/img/azure_ml_01-02-workspace.png" alt="workspace"/></p> <p>The Azure resources created alongside a workspace include:</p> <ul> <li>A storage account - used to store files used by the workspace as well as data for experiments and model training.</li> <li>An Application Insights instance, used to monitor predictive services in the workspace.</li> <li>An Azure Key Vault instance, used to manage secrets such as authentication keys and credentials used by the workspace.</li> <li>A container registry, created as-needed to manage containers for deployed models.</li> </ul> <p>For each one of the resources listed above there are associated costs even is the workspace in not used. So you must delete the worspace id you do not want to consume your credits1.</p> <h2 id="compute-resources">Compute Resources</h2> <p>There are four kinds of compute resource you can create:</p> <ul> <li>Compute Instances: Development workstations that data scientists can use to work with data and models.</li> <li>Compute Clusters: Scalable clusters of virtual machines for on-demand processing of experiment code.</li> <li>Inference Clusters: Deployment targets for predictive services that use your trained models.</li> <li>Attached Compute: Links to existing Azure compute resources, such as Virtual Machines or Azure Databricks clusters.</li> </ul> <h3 id="creating-a-workspace">Creating a Workspace</h3> <p>You can create a workspace in any of the following ways:</p> <ol> <li>In the Microsoft Azure portal, create a new Machine Learning resource, specifying the subscription, resource group and workspace name.</li> <li>Use the Azure Machine Learning Python SDK to run code that creates a workspace.</li> </ol> <p>For example, the following code creates a workspace named aml-workspace (assuming the Azure ML SDK for Python is installed and a valid subscription ID is specified):</p> <pre><code class="language-Python">from azureml.core import Workspace
    
    ws = Workspace.create(name='aml-workspace', 
                      subscription_id='123456-abc-123...',
                      resource_group='aml-resources',
                      create_resource_group=True,
                      location='eastus'
                     )
</code></pre> <ol> <li>Use the Azure Command Line Interface (CLI) with the Azure Machine Learning CLI extension.</li> </ol> <p>For example, you could use the following command (which assumes a resource group named aml-resources has already been created):</p> <pre><code class="language-Python">az ml workspace create -w 'aml-workspace' -g 'aml-resources'
</code></pre> <h2 id="azure-machine-learning-studio">Azure Machine Learning studio</h2> <p>Azure Machine Learning studio is a web-based tool for managing an Azure Machine Learning workspace.</p> <p><img src="/assets/img/azure_ml-01-03-aml-studio.jpg" alt="Azure Machine Learnign Studio"/></p> <p>To use Azure Machine Learning studio, use a a web browser to navigate to <a href="https://ml.azure.com">https://ml.azure.com</a> and sign in using credentials associated with your Azure subscription. You can then select the subscription and workspace you want to manage.</p> <p>A previously released tool named Azure Machine Learning Studio provided a free service for drag and drop machine learning model development. The studio interface for the Azure Machine Learning service includes this capability in the designer tool, as well as other workspace asset management capabilities. The old tool still availale as ‚ÄúAzure Machine Learning Studio <strong>Classic</strong>‚Äù.</p> <p>Azure Machine Learning Studio Classic miss many features present in Azure Machine Learning Worspace, but there is 100% free service plan available.</p> <h2 id="the-azure-machine-learning-sdk">The Azure Machine Learning SDK</h2> <p>While graphical interfaces like Azure Machine Learning studio make it easy to create and manage machine learning assets, it is often advantageous to use a code-based approach to managing resources.</p> <p>Azure Machine Learning provides software development kits (SDKs) for Python and R, which you can use to create, manage, and use assets in an Azure Machine Learning workspace.</p> <h3 id="installing-the-azure-machine-learning-sdk-for-python">Installing the Azure Machine Learning SDK for Python</h3> <p>You can install the Azure Machine Learning SDK for Python by using the pip package management utility.</p> <pre><code class="language-Python">pip install azureml-sdk[notebooks,automl,explain]
</code></pre> <p>For more information about installing the Azure Machine Learning SDK for Python, see the <a href="https://aka.ms/AA70rq7">SDK documentation</a>.</p> <h3 id="retrieving-and-viewing-logged-metrics">Retrieving and Viewing Logged Metrics</h3> <p>You can view the metrics logged by an experiment run in Azure Machine Learning studio or by using the RunDetails widget in a notebook, as shown here:</p> <pre><code class="language-Python">from azureml.widgets import RunDetails

RunDetails(run).show()

</code></pre> <pre><code class="language-Python">import json

# Get logged metrics
metrics = run.get_metrics()
print(json.dumps(metrics, indent=2))

</code></pre> <h2 id="experiment-output-files">Experiment Output Files</h2> <p>In addition to logging metrics, an experiment can generate output files. Often these are trained machine learning models, but you can save any sort of file and make it available as an output of your experiment run. The output files of an experiment are saved in its outputs folder.</p> <pre><code class="language-Python">run.upload_file(name='outputs/sample.csv', path_or_stream='./sample.csv')
</code></pre> <p>When running an experiment in a remote compute context, any files written to the outputs folder in the compute context are automatically uploaded to the run‚Äôs outputs folder when the run completes.</p> <p>Whichever approach you use to run your experiment, you can retrieve a list of output files from the Run object like this:</p> <pre><code class="language-Python">import json

files = run.get_file_names()
print(json.dumps(files, indent=2))
</code></pre> <h2 id="running-a-script-as-an-experiment">Running a Script as an Experiment</h2> <p>To run a script as an experiment, you must define a script configuration that defines the script to be run and the Python environment in which to run it. This is implemented by using a ScriptRunConfig object.</p> <p>For example, the following code could be used to run an experiment based on a script in the experiment_files folder</p> <pre><code class="language-Python">from azureml.core import Experiment, ScriptRunConfig

# Create a script config
script_config = ScriptRunConfig(source_directory=experiment_folder,
                                script='experiment.py') 

# submit the experiment
experiment = Experiment(workspace = ws, name = 'my-experiment')
run = experiment.submit(config=script_config)
run.wait_for_completion(show_output=True)

</code></pre> <p>An experiment script (such as experiment.py) is just a Python code file that contains the code you want to run in the experiment. To access the experiment run context (which is needed to log metrics) the script must import the <strong>azureml.core.Run</strong> class and call its get_context method. The script can then use the run context to log metrics, upload files, and complete the experiment.</p> <pre><code class="language-Python">from azureml.core import Run
import pandas as pd
import matplotlib.pyplot as plt
import os

# Get the experiment run context
run = Run.get_context()

# load the diabetes dataset
data = pd.read_csv('data.csv')

# Count the rows and log the result
row_count = (len(data))
run.log('observations', row_count)

# Save a sample of the data
os.makedirs('outputs', exist_ok=True)
data.sample(100).to_csv("outputs/sample.csv", index=False, header=True)

# Complete the run
run.complete()

</code></pre>]]></content><author><name></name></author><category term="research"/><category term="Azure"/><category term="AI"/><category term="ML"/><category term="Azure"/><category term="ML"/><category term="MLOPs"/><summary type="html"><![CDATA[Azure Machine Learning is a cloud-based platform for building and operating machine learning solutions in Azure.]]></summary></entry><entry><title type="html">Azure Machine Learning - Hyperparameter Tuning</title><link href="https://emanubuc.github.io/blog/2021/Hyperparameter-tuning/" rel="alternate" type="text/html" title="Azure Machine Learning - Hyperparameter Tuning"/><published>2021-03-09T09:08:00+00:00</published><updated>2021-03-09T09:08:00+00:00</updated><id>https://emanubuc.github.io/blog/2021/Hyperparameter-tuning</id><content type="html" xml:base="https://emanubuc.github.io/blog/2021/Hyperparameter-tuning/"><![CDATA[<h1 id="azure-machine-learning---hyperparameter-tuning">Azure Machine Learning - Hyperparameter Tuning</h1> <p>In machine learning, models are trained to predict unknown labels for new data based on correlations between known labels and features found in the training data. Depending on the algorithm used, you may need to specify hyperparameters to configure how the model is trained.</p> <p>For example, the logistic regression algorithm uses a regularization rate hyperparameter to counteract overfitting; and deep learning techniques for convolutional neural networks (CNNs) use hyperparameters like learning rate to control how weights are adjusted during training, and batch size to determine how many data items are included in each training batch.</p> <p>Data scientists refer to the values determined from the training features as <em>parameters</em>, <strong>values that are used to configure training behavior but which are not derived from the training data</strong> are named <em>hyperparameter</em>.</p> <p>The choice of hyperparameter values can significantly affect the resulting model, making it important to select the best possible values.</p> <p>Hyperparameter tuning is accomplished by training the multiple models, using the same algorithm and training data but different hyperparameter values. The resulting model from each training run is then evaluated to determine the performance metric for which you want to optimize (for example, <em>accuracy</em>), and the best-performing model is selected.</p> <p>In Azure Machine Learning, you achieve this through an <em>experiment</em> that consists of a <em>hyperdrive run</em>, which initiates a child run for each hyperparameter combination to be tested. Each child run uses a training script with parameterized hyperparameter values to train a model, and logs the target performance metric achieved by the trained model.</p> <p>Hyperdrive run configuration must include:</p> <ul> <li>hyperparameter search space.</li> <li>hyperparameter sampling.</li> <li>early-termination policy.</li> </ul> <h2 id="sampling">Sampling</h2> <h3 id="grid-sampling">Grid Sampling</h3> <p>Grid Sampling is used to try every possible combination of parameters in the search space.</p> <pre><code class="language-Python">from azureml.train.hyperdrive import GridParameterSampling, choice

param_space = {
                 '--batch_size': choice(16, 32, 64),
                 '--learning_rate': choice(0.01, 0.1, 1.0)
              }

param_sampling = GridParameterSampling(param_space)

</code></pre> <h3 id="random-samplig">Random Samplig</h3> <p>Random sampling is used to randomly select a value for each hyperparameter</p> <pre><code class="language-Python">from azureml.train.hyperdrive import RandomParameterSampling, choice, normal

param_space = {
                 '--batch_size': choice(16, 32, 64),
                 '--learning_rate': normal(10, 3)
              }

param_sampling = RandomParameterSampling(param_space)

</code></pre> <h3 id="bayesian-sampling">Bayesian sampling</h3> <p>Bayesian sampling chooses hyperparameter values based on the Bayesian optimization algorithm, which tries to select parameter combinations that will result in improved performance from the previous selection.</p> <pre><code class="language-Python">from azureml.train.hyperdrive import BayesianParameterSampling, choice, uniform

param_space = {
                 '--batch_size': choice(16, 32, 64),
                 '--learning_rate': uniform(0.05, 0.1)
              }

param_sampling = BayesianParameterSampling(param_space)
</code></pre> <p>Note: Bayesian sampling can be used only with choice, uniform, and quniform parameter expressions, and can not be combined with an early-termination policy</p> <h2 id="early-termination-policy">Early Termination Policy</h2> <p>With a sufficiently large hyperparameter search space, it could take many iterations (i.e. child runs) to try every possible combination. To prevent wasting time and money, in addition to a maximum number of rins, you can set an <strong>early termination policy</strong> that abandons runs that are unlikely to produce a better result than previously completed runs.</p> <p>The policy is evaluated at an <code class="language-plaintext highlighter-rouge">evaluation_interval</code> you specify. You can also set a <code class="language-plaintext highlighter-rouge">delay_evaluation</code> parameter to avoid evaluating the policy until a minimum number of iterations have been completed.</p> <p>Early termination is particularly useful for deep learning scenarios where a deep neural network (DNN) is trained iteratively over a number of epochs. The training script can report the target metric after each epoch, and if the run is significantly underperforming previous runs after the same number of intervals, it can be abandoned.</p> <h3 id="bandit-policy">Bandit Policy</h3> <p>Use a bandit policy to stop a run if the target performance metric underperforms the best run so far by a specified margin expressed as absolute value (<code class="language-plaintext highlighter-rouge">slack_amount</code>) or factor (<code class="language-plaintext highlighter-rouge">slack_factor</code>)</p> <p>This example applies the policy for every iteration after the first five, and abandons runs where the reported target metric is 0.2 or more worse than the best performing run after the same number of intervals.</p> <pre><code class="language-Python">from azureml.train.hyperdrive import BanditPolicy

early_termination_policy = BanditPolicy(slack_amount = 0.2,
                                        evaluation_interval=1,
                                        delay_evaluation=5)
</code></pre> <p>You can also apply a bandit policy using a slack factor, which compares the performance metric as a ratio rather than an absolute value.</p> <h3 id="median-stopping-policy">Median Stopping Policy</h3> <p>A median stopping policy abandons runs where the target performance metric is worse than the median of the running averages for all runs.</p> <pre><code class="language-Python">from azureml.train.hyperdrive import MedianStoppingPolicy

early_termination_policy = MedianStoppingPolicy(evaluation_interval=1,
                                                delay_evaluation=5)
</code></pre> <h3 id="truncation-selection-policy">Truncation selection policy</h3> <p>A truncation selection policy cancels the lowest performing X% of runs at each evaluation interval based on the truncation_percentage value you specify for X.</p> <pre><code class="language-Python">from azureml.train.hyperdrive import TruncationSelectionPolicy

early_termination_policy = TruncationSelectionPolicy(truncation_percentage=10,
                                                     evaluation_interval=1,
                                                     delay_evaluation=5)

</code></pre> <h2 id="running-a-hyperparameter-tuning-experiment">Running a hyperparameter tuning experiment</h2> <p>In Azure Machine Learning, you can tune hyperparameters by running a hyperdrive experiment.</p> <h3 id="training-script-for-hyperparameter-tuning">Training script for hyperparameter tuning</h3> <p>To run a hyperdrive experiment, you need to create a training script just the way you would do for any other training experiment, except that your script must:</p> <ul> <li>Include an argument for each hyperparameter you want to vary.</li> <li>Log the target performance metric. This enables the hyperdrive run to evaluate the performance of the child runs it initiates, and identify the one that produces the best performing model.</li> </ul> <p>You can found an example of HyperDrive experiment and a <a href="https://github.com/emanbuc/Optimizing_a_Pipeline_in_AzureML/blob/master/train.py">training script</a> in the ‚ÄúOptimizing a Pipaline in AzureML‚Äù <a href="https://github.com/emanbuc/Optimizing_a_Pipeline_in_AzureML">GitHub repository</a></p> <p>A reference training script example you can use as template is the following one:</p> <pre><code class="language-Python"># using a --regularization argument to set the regularization rate 
# hyperparameter, and logs the accuracy metric with the name Accuracy

import argparse
import joblib
from azureml.core import Run
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

# Get regularization hyperparameter
parser = argparse.ArgumentParser()
parser.add_argument('--regularization', type=float, dest='reg_rate', default=0.01)
args = parser.parse_args()
reg = args.reg_rate

# Get the experiment run context
run = Run.get_context()

# load the training dataset
data = run.input_datasets['training_data'].to_pandas_dataframe()

# Separate features and labels, and split for training/validatiom
X = data[['feature1','feature2','feature3','feature4']].values
y = data['label'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)

# Train a logistic regression model with the reg hyperparameter
model = LogisticRegression(C=1/reg, solver="liblinear").fit(X_train, y_train)

# calculate and log accuracy
y_hat = model.predict(X_test)
acc = np.average(y_hat == y_test)
run.log('Accuracy', np.float(acc))

# Save the trained model
os.makedirs('outputs', exist_ok=True)
joblib.dump(value=model, filename='outputs/model.pkl')

run.complete()

</code></pre> <h3 id="configuring-hyperdrive-experiment">Configuring hyperdrive experiment</h3> <p>To configure HyperDrive you must use HyperDriveConfig class as shown in the following example:</p> <p>from azureml.core import Experiment from azureml.train.hyperdrive import HyperDriveConfig, PrimaryMetricGoal</p> <pre><code class="language-Python">
# Assumes ws, script_config and param_sampling are already defined
hyperdrive = HyperDriveConfig(run_config=script_config,
                              hyperparameter_sampling=param_sampling,
                              policy=None,
                              primary_metric_name='Accuracy',
                              primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,
                              max_total_runs=6,
                              max_concurrent_runs=4)

experiment = Experiment(workspace = ws, name = 'hyperdrive_training')
hyperdrive_run = experiment.submit(config=hyperdrive)
</code></pre> <h3 id="monitoring-and-reviewing-hyperdrive-runs">Monitoring and reviewing hyperdrive runs</h3> <p>You can monitor hyperdrive experiments in Azure Machine Learning studio, or by using the Jupyter Notebooks RunDetails widget.</p> <p><img src="/assets/img/hyperdrive_monitoring-experiment.png" alt="monitoring experiment"/></p>]]></content><author><name></name></author><category term="research"/><category term="Azure"/><category term="AI"/><category term="ML"/><category term="Azure"/><category term="ML"/><category term="MLOPs"/><summary type="html"><![CDATA[The choice of hyperparameter values can significantly affect the resulting model, making it important to select the best possible values.]]></summary></entry><entry><title type="html">Machine Learning and Big Data Course Outline</title><link href="https://emanubuc.github.io/blog/2021/Machine_Learning_And_Big_Data/" rel="alternate" type="text/html" title="Machine Learning and Big Data Course Outline"/><published>2021-03-08T10:08:00+00:00</published><updated>2021-03-08T10:08:00+00:00</updated><id>https://emanubuc.github.io/blog/2021/Machine_Learning_And_Big_Data</id><content type="html" xml:base="https://emanubuc.github.io/blog/2021/Machine_Learning_And_Big_Data/"><![CDATA[<h1 id="machine-learning-and-big-data">Machine Learning and Big Data</h1> <h2 id="table-of-content">Table of Content</h2> <ul> <li>Big Data Meet ML <ul> <li>What is ML</li> <li>What is Big Data</li> <li>For Big Data Analysis we need Machine Learning</li> <li>For ML we need Big Data</li> <li>ML for data analysis in physics and other sciences</li> </ul> </li> <li> <p>AI at Scale</p> </li> <li>ML on Big Data <ul> <li>Big Data -&gt; Big Opportunities</li> <li>Big Data -&gt; Big Challenges</li> <li>Survey of Machine Learning for Big DataProcessing</li> </ul> </li> <li>Distributed ML Training <ul> <li>Tools and Techniques for distributed Training</li> <li>Data parallelism / Model parallelism</li> </ul> </li> <li>Compute resources for ML <ul> <li>compute hardware for ML</li> <li>local vs cloud</li> <li>Why cloud</li> <li>When (not) cloud</li> <li>Cloud platform for ML</li> <li>Notebook server ad Full server solution</li> <li>Notebook: Colab, Gradient</li> <li>Full serer: Google Cloud, Azure</li> </ul> </li> <li>A Close look at Azure Machine Learning <ul> <li>Azure for student (with @unipg.it account)</li> <li>Azure ML Platform</li> <li>Azure ML Studio</li> <li>Azure ML SDK</li> <li>Training Workflow</li> <li>ML Pipeline / ML OPS</li> <li>Deploy</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="Courses"/><category term="AI"/><category term="ML"/><category term="BigData"/><category term="AI"/><category term="ML"/><category term="BigData"/><summary type="html"><![CDATA[Machine Learning and Big Data lecture topics]]></summary></entry></feed>