<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://emanubuc.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://emanubuc.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-29T10:54:25+00:00</updated><id>https://emanubuc.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html"></title><link href="https://emanubuc.github.io/blog/2025/2025-07-11-Online_latex_editor/" rel="alternate" type="text/html" title=""/><published>2025-12-29T10:54:25+00:00</published><updated>2025-12-29T10:54:25+00:00</updated><id>https://emanubuc.github.io/blog/2025/2025-07-11-Online_latex_editor</id><content type="html" xml:base="https://emanubuc.github.io/blog/2025/2025-07-11-Online_latex_editor/"><![CDATA[<h1 id="online-latex-editors-and-collaboration-tools-for-academic-writing-features-pricing-and-security">Online LaTeX Editors and Collaboration Tools for Academic Writing: Features, Pricing, and Security</h1> <p><strong>Date:</strong> September 11, 2025</p> <h2 id="abstract">Abstract</h2> <p>This post provides a comprehensive analysis of online LaTeX editors and collaboration tools designed for academic writing. We examine leading platforms including Overleaf, Papeeria, Authorea, and other alternatives, comparing their features, pricing models, and collaborative capabilities. Special attention is given to available AI powered writing assistant and to data security and privacy compliance (particularly GDPR) for protecting sensitive academic content. Users should choose the solution that best fits requirements and budget constrains. Our analysis reveals that while in online LaTeX editors offer significant advantages for collaborative academic writing, institutions must carefully evaluate security implications when handling sensitive research data. Different set of features are offered across a tiered pricing model, ranging from robust free plans to professional tiers costing up to 40 €/month.</p> <h2 id="introduction">Introduction</h2> <p>LaTeX has become the gold standard for academic and scientific document preparation, especially in fields requiring complex notation, precise formatting, and strong bibliographic management. The evolution from desktop to cloud-based collaborative platforms has revolutionized research workflows.</p> <p>Online LaTeX editors combine powerful typesetting with modern collaboration tools, enabling real-time editing, version control, and sharing across teams.</p> <h2 id="online-latex-editors">Online LaTeX Editors</h2> <p>The most popular and widely used online LaTeX editor is <em>Overleaf</em>, known for its collaborative features, real-time editing, document sharing, and extensive template library, making it a standard tool for academic and technical writing worldwide. Other popular online editors include <em>Papeeria</em> (https://papeeria.com) and <em>Authorea</em> (https://www.authorea.com). Beyond the most popular and established platforms, a new generation of online LaTeX editors is gaining traction. Also cloud-based managed computation platforms such as <em>CoCalc</em> (https://cocalc.com/) that include a LaTeX editor should be considered.</p> <h3 id="getting-started">Getting started</h3> <p>All online LaTeX featured a very friendly on-boarding process that allow the user to start writing in less than 2 minutes. Only an e-mail address is required activate a free plan instance. <em>CoCalc</em>, due to the different nature of the platform, require some more effort for the initial setup and do not provide a free plan.</p> <h3 id="overleaf">Overleaf</h3> <p>Overleaf is the dominant player, now serving over 17 million users. After merging with ShareLaTeX, it offers:</p> <ul> <li> <p>Real-time collaborative editing</p> </li> <li> <p>400+ academic templates</p> </li> <li> <p>Integrated PDF view and SyncTeX support</p> </li> <li> <p>Advanced bibliography management</p> </li> <li> <p>Git integration and document history</p> </li> <li> <p>Track changes and commenting system</p> </li> </ul> <p>Integration includes reference management tools (Zotero, Mendeley) and services like Dropbox and GitHub. Overleaf partners with Paperpal for grammar checking tailored to LaTeX documents.</p> <h3 id="papeeria">Papeeria</h3> <p>Papeeria emphasizes simplicity but retains robust LaTeX functionality. Unique features include:</p> <ul> <li> <p>Real-time collaboration</p> </li> <li> <p>Integrated plotting with gnuplot</p> </li> <li> <p>Version control and Git sync</p> </li> <li> <p>Auto-compiling on paid tiers</p> </li> <li> <p>Mendeley integration for references</p> </li> </ul> <p>Pricing: Free plan includes unlimited public projects, the Delta plan (USD 5/month) unlocks private projects and advanced features. Verified students get Delta features free.</p> <h3 id="authorea">Authorea</h3> <p>Authorea, acquired by Wiley, goes beyond LaTeX and supports:</p> <ul> <li> <p>Data integration (embedding datasets, plotting)</p> </li> <li> <p>Executable code blocks and Jupyter notebooks</p> </li> <li> <p>Journal-specific formatting for 40+ publishers</p> </li> <li> <p>Pre-print workflow integration</p> </li> </ul> <p>Best for computational research requiring close integration between text and data.</p> <h3 id="emerging-alternatives">Emerging Alternatives</h3> <h4 id="crixet">Crixet</h4> <p>Home site: https://crixet.com Crixet delivers a cross-browser, cross-device experience with LaTeX-aware intelligent autocompletion and inline error detection. Its standout feature, Chirp, functions as an AI co-pilot: users can type, speak, or even sketch ideas to generate LaTeX code, graphs, or tables seamlessly. Crixet also integrates fast citation search—tightly coupled with Zotero—and supports one-click refactoring of code into modular files. A forthcoming on-premise edition promises full feature parity with the cloud service, enabling organizations to host Crixet within their own infrastructure for maximum compliance and data control…. #### TeXPage</p> <p>Home site: https://www.texpage.com TeXPage focuses on simplicity and security with end-to-end encrypted transmission and encrypted at-rest storage, along with multiple redundant backups. Its real-time collaboration supports synchronous editing, asynchronous workflows, and rich commenting with track-change visibility. Beyond standard LaTeX editing, TeXPage offers a visual formula editor, auto-generating tables via an intuitive GUI, and a dynamic file outline panel for quick navigation. Version history is fully transparent, allowing users to restore earlier drafts at any time. TeXPage’s commitment to encrypted workflows makes it especially appealing for institutions with strict data-privacy mandates.</p> <h4 id="inscriveio">inscrive.io</h4> <p>Home site: https://inscrive.io Positioning itself as the only fully GDPR-compliant cloud LaTeX editor, inscrive.io emphasizes performance and unmetered compilation. Collaborators enjoy real-time editing with unlimited participants, backed by extensive version control and AI-accelerated compile speeds. By guaranteeing EU-based data processing and compliance with the latest privacy standards, inscrive.io offers peace of mind to European universities and research consortia. Its streamlined interface prioritizes fast loading times and minimal configuration, so users can focus on content creation without infrastructure concerns.</p> <h3 id="ai-support">AI Support</h3> <p>Modern online LaTeX platforms are increasingly embedding AI-powered assistants to streamline writing, debugging, and formatting workflows. By leveraging these AI assistants, researchers can focus more on substantive content and less on low-level formatting, making LaTeX more accessible to novices and veterans alike. Below are several concrete implementations.</p> <p><strong>Writefull</strong> (an add-on for Overleaf) offers AI-powered language assistance tailored for academic writing.</p> <h4 id="intelligent-autocompletion-and-error-detection">Intelligent Autocompletion and Error Detection</h4> <p>Crixet’s “Chirp” co-pilot (https://crixet.com/) uses AI to offer context-aware suggestions for macros, environments, and symbols, plus inline error detection that highlights compilation issues in real time.</p> <h4 id="natural-language-to-latex-conversion">Natural Language to LaTeX Conversion</h4> <p>Overleaf’s integration with Paperpal (https://paperpal.com/) lets users describe content in plain English—such as “a quadratic formula”—and receive fully formatted LaTeX code in seconds.</p> <h4 id="voice-and-sketch-driven-content-creation">Voice and Sketch-Driven Content Creation</h4> <p>Crixet (https://crixet.com/) allows voice dictation or freehand sketches to generate LaTeX code and vector graphics, accelerating drafts for users less familiar with raw syntax.</p> <h4 id="grammar-and-style-coaching">Grammar and Style Coaching</h4> <p>Writefull AI (https://writefull.com/) integrates directly into Overleaf, providing academic-style grammar checks, passive-voice alerts, and rephrasing suggestions that respect LaTeX document structure.</p> <h4 id="citation-and-bibliography-automation">Citation and Bibliography Automation</h4> <p>TeXPage (https://www.texpage.com/) features AI-driven citation lookup with one-click DOI metadata retrieval, seamless Zotero integration, and automatic bibliography formatting for both numbered and author–year styles.</p> <h4 id="collaborative-ai-driven-review">Collaborative AI-Driven Review</h4> <p>inscrive.io (https://inscrive.io/) employs AI to summarize recent edits, flag unresolved comments, and recommend missing citations or clarifications based on document content and version history.</p> <h2 id="feature-comparison">Feature Comparison</h2> <p>When selecting an online editor, you should consider your needs, requirements (such as security, privacy, data residency), budget constraints. The available solutions should be compared along the following dimensions:</p> <ol> <li> <p>Editing Capabilities and additional tools</p> </li> <li> <p>Collaboration</p> </li> <li> <p>Specific tools for academic writing (templates, journal submission automation)</p> </li> <li> <p>Simplicity</p> </li> <li> <p>Integration with external platform</p> </li> <li> <p>Cost and pricing plan available (including discounted offers for students or other categories)</p> </li> <li> <p>Resource limitations associated with the chosen plan such as compilation time, memory and storage limits</p> </li> </ol> <h3 id="editing-capabilities">Editing Capabilities</h3> <p>All major platforms provide syntax highlighting, autocompletion, and error detection, but differ in specialist features:</p> <ol> <li> <p>Overleaf: Most comprehensive mathematical palette</p> </li> <li> <p>Papeeria: In-editor plotting tools</p> </li> <li> <p>Authorea: Executable content (notebooks)</p> </li> </ol> <h3 id="templates">Templates</h3> <p>Overleaf: 400+ templates spanning papers, dissertations, CVs; Papeeria: Focused, specialist templates; Authorea: Journal-specific.</p> <h3 id="collaboration-all-offer-simultaneous-multi-user-editing-overleaf-has-conflict-resolution-and-version-history-papeeria-offers-discussion-features-authorea-emphasizes-data-sharing">Collaboration… All offer simultaneous multi-user editing. Overleaf has conflict resolution and version history; Papeeria offers discussion features; Authorea emphasizes data-sharing.</h3> <h3 id="integration">Integration</h3> <p>Reference management (Zotero, Mendeley, Papers) and cloud sync (Dropbox, Google Drive, GitHub) vary by platform. The value of these features depend on your personal and team workflow.</p> <h2 id="pricing-comparison">Pricing Comparison</h2> <h3 id="overleaf-1">Overleaf</h3> <p>Free: Basic editing, 1 collaborator/project Standard ($199/year): 10 collaborators, history, advanced features Professional ($399/year): Unlimited collaborators, priority compile Institutional: Site-wide licenses with SSO, admin controls</p> <h3 id="papeeria-1">Papeeria</h3> <p>Delta plan: 5 USD /month, lower cost, most core features included</p> <p>Free premium for students</p> <h3 id="authorea-1">Authorea</h3> <p>Free for public docs; paid for private/research features</p> <h2 id="data-security-and-privacy">Data Security and Privacy</h2> <h3 id="gdpr-compliance">GDPR Compliance</h3> <p>EU institutions require platforms to meet GDPR for data processing, retention, deletion, and user control. Overleaf processes data in EU/US sites, is ISO/IEC 27001 certified, and meets data privacy framework requirements.</p> <h3 id="technical-safeguards">Technical Safeguards</h3> <p>All use HTTPS/TLS, encryption at rest, containerization, regular backups.</p> <h3 id="academic-risks">Academic Risks</h3> <p>Sensitive/unpublished content, patent material, or personal health data should be handled per institutional protocols. Platforms have had vulnerabilities in the past (e.g., ShareLaTeX command execution flaws), now mitigated with sandboxing and improved filtering.</p> <h3 id="security-best-practices">Security Best Practices</h3> <p>Institutions should:</p> <ul> <li> <p>Train users on sensitive content recognition</p> </li> <li> <p>Implement SSO/access controls</p> </li> <li> <p>Audit usage and backups</p> </li> <li> <p>Employ hybrid models for sensitive research</p> </li> </ul> <h2 id="comparative-recommendations">Comparative Recommendations</h2> <p>Based on the present survey and 5 years of daily user experience my favorite tools is <em>Overleaf</em>, but it is also the most expensive. I found a better features versus cost balance in <em>TeXPage</em> and <em>www.papeeria.com</em> 5 € per month plans. My general advises are:</p> <ul> <li> <p>If your research institution provide institutional account probably you should take advantage of this benefit.</p> </li> <li> <p>Create a free account, import a project representative of you typical paper a try to work on the project for at lest few hours. Some days is better if possible.</p> </li> <li> <p>If you are a <em>privacy-focused</em> user (or you have to work under security constrains) consider on-premises solutions or at least an hybrid solution with most sensitive data keep in private server while using a public cloud based collaboration platform. EU based platform and data center may also help with security and privacy requirements.</p> </li> </ul> <h2 id="future-trends">Future Trends</h2> <p>AI integration (grammar, citation management, formula generation) are rising, with privacy risks to consider. End-to-end encryption, decentralized models, and zero-trust architectures are on the horizon.</p> <h2 id="conclusion">Conclusion</h2> <p>Online LaTeX editors have radically improved academic writing workflows, with Overleaf leading in features and adoption. Data security and institutional policy remain central: hybrid approaches, regular audits, and robust training enable safe, productive research collaboration.</p>]]></content><author><name></name></author></entry><entry><title type="html">Dove pubblicare i tuoi tutorial e dispense di informatica</title><link href="https://emanubuc.github.io/blog/2025/Dove_pubblicare_tutorial_e_dispense_informatica/" rel="alternate" type="text/html" title="Dove pubblicare i tuoi tutorial e dispense di informatica"/><published>2025-10-03T11:46:00+00:00</published><updated>2025-10-03T11:46:00+00:00</updated><id>https://emanubuc.github.io/blog/2025/Dove_pubblicare_tutorial_e_dispense_informatica</id><content type="html" xml:base="https://emanubuc.github.io/blog/2025/Dove_pubblicare_tutorial_e_dispense_informatica/"><![CDATA[<h1 id="dove-pubblicare-i-tuoi-tutorial-e-dispense-di-informatica-quattro-piattaforme-per-citabilità-e-riconoscimento">Dove pubblicare i tuoi tutorial e dispense di informatica: quattro piattaforme per citabilità e riconoscimento</h1> <p>Hai scritto delle <strong>dispense</strong> dettagliate su machine learning, un <strong>tutorial</strong> su Python o una <strong>guida tecnica</strong> su Kubernetes? Ottimo!</p> <p>Prima di limitarti a un PDF sul tuo sito personale, a un post sul tuo blog o a un articolo su Medium, sappi che esistono piattaforme pensate per il mondo accademico e tecnico che ti permettono non solo di raggiungere una community mirata, ma anche di rendere il tuo lavoro permanentemente citabile da altri ricercatori e professionisti.</p> <p>Oltre a scrivere un post su Medium, è utile pubblicare il tuo lavoro su una piattaforma che garantisca la <strong>citabilità permanente</strong> (tramite DOI o ID persistente) e l’<strong>indicizzazione</strong> nei motori di ricerca accademici.</p> <p>Se pensi di pubblicare lo stesso materiale (o anche solo una parte di esso) su una rivista accademica, verifica prima la politica dell’editore (o della specifica rivista) sui cosiddetti <em>preprint</em>. Molte riviste accettano articoli che sono stati precedentemente pubblicati su preprint server riconosciuti, ma è sempre meglio controllare le regole specifiche della rivista.</p> <p>Di seguito trovi quattro ottime opzioni, classificate in base al loro focus disciplinare e alla tipologia di materiale che accettano.</p> <hr/> <h2 id="1-techrxiv-lautorità-tecnica-per-ingegneria-e-informatica-ieee">1. TechRxiv: l’autorità tecnica per ingegneria e informatica (IEEE)</h2> <p>Se le tue dispense riguardano il machine learning, la robotica o l’ingegneria elettrica, <strong>TechRxiv</strong> è una scelta mirata.</p> <ul> <li><strong>Cos’è:</strong> un server di preprint moderato, <strong>supportato dall’IEEE</strong> (Institute of Electrical and Electronics Engineers).</li> <li><strong>Visibilità &amp; indicizzazione:</strong> eccellente — è riconosciuto e indicizzato da <strong>Google Scholar</strong>, quindi il tuo lavoro sarà facilmente reperibile dalla comunità accademica di settore.</li> <li><strong>Citabilità:</strong> assegna un <strong>identificatore persistente</strong> che permette la citazione formale.</li> <li><strong>Ideale per:</strong> dispense di ML, report tecnici, guide avanzate su hardware o protocolli.</li> </ul> <hr/> <h2 id="2-arxiv-la-vetrina-della-ricerca-pura-massima-diffusione">2. arXiv: la vetrina della ricerca pura (massima diffusione)</h2> <p>Se le tue dispense sono strutturate come un <strong>survey</strong> o un <strong>report tecnico</strong> con l’obiettivo di raggiungere i ricercatori, prendi in considerazione <strong>arXiv</strong>.</p> <ul> <li><strong>Cos’è:</strong> il server di preprint originale e più grande, ampiamente usato nelle comunità di fisica, matematica e computer science (es. <code class="language-plaintext highlighter-rouge">cs.LG</code> per machine learning).</li> <li><strong>Visibilità &amp; indicizzazione:</strong> massima — è lo standard de facto per la diffusione rapida della ricerca e gode di eccellente indicizzazione su Google Scholar.</li> <li><strong>Citabilità:</strong> utilizza un identificatore unico (<code class="language-plaintext highlighter-rouge">arXiv:YYMM.#####</code>) universalmente riconosciuto.</li> <li><strong>Formato:</strong> il processo di submission richiede solitamente i sorgenti LaTeX.</li> <li><strong>Attenzione:</strong> arXiv è orientato alla ricerca originale; report e survey sono accettati se i moderatori li ritengono scientificamente rilevanti.</li> </ul> <hr/> <h2 id="3-zenodo-larchivio-flessibile-doi-e-archiviazione-multi-formato">3. Zenodo: l’archivio flessibile (DOI e archiviazione multi-formato)</h2> <p>Se cerchi la soluzione più semplice che garantisca un <strong>DOI</strong> e permetta di archiviare vari materiali, <strong>Zenodo</strong> è molto indicato.</p> <ul> <li><strong>Cos’è:</strong> un repository ad accesso aperto gestito dal <strong>CERN</strong>; accetta qualsiasi output della ricerca, inclusi report, software, dataset e materiale didattico.</li> <li><strong>Visibilità &amp; indicizzazione:</strong> buona — indicizzato da Google Search e Google Dataset Search; l’indicizzazione in Google Scholar non è automatica come per arXiv o TechRxiv, ma il DOI garantisce la citabilità.</li> <li><strong>Citabilità:</strong> assegna un <strong>DOI (Digital Object Identifier)</strong> a ogni pubblicazione.</li> <li><strong>Ideale per:</strong> dispense accompagnate da codice sorgente o dataset (tutto in un unico record DOI).</li> </ul> <hr/> <h2 id="4-ssrn-la-rete-di-lavoro-accademico-focus-su-working-paper">4. SSRN: la rete di lavoro accademico (focus su working paper)</h2> <p>SSRN è un’opzione valida soprattutto se il tuo lavoro si colloca all’intersezione tra informatica e altre discipline come economia o diritto.</p> <ul> <li><strong>Cos’è:</strong> nato per le scienze sociali, oggi di proprietà di Elsevier, si è ampliato includendo anche ambiti di computer science e engineering; è focalizzato sui <strong>working paper</strong>.</li> <li><strong>Visibilità &amp; indicizzazione:</strong> molto buona — particolarmente diffuso negli Stati Uniti per la diffusione preliminare della ricerca.</li> <li><strong>Citabilità:</strong> fornisce un <strong>identificatore persistente</strong> ed è integrato con varie metriche accademiche.</li> <li><strong>Ideale per:</strong> documenti in stile “report accademico” o working paper, o lavori con forte intersezione con scienze sociali.</li> </ul> <hr/> <h3 id="quale-scegliere">Quale scegliere?</h3> <p>La scelta dipende dall’argomento, dallo scopo della pubblicazione e dal tipo di materiale (documento LaTeX, documento Word, file con dataset, codice sorgente, software associato, ecc.). Esempio per dispense di machine learning già pronte in LaTeX/PDF:</p> <table> <thead> <tr> <th style="text-align: left">Scenario</th> <th style="text-align: left">Piattaforma raccomandata</th> <th style="text-align: left">Motivo principale</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Priorità: visibilità accademica e reputazione settoriale</strong></td> <td style="text-align: left"><strong>TechRxiv</strong></td> <td style="text-align: left">Supportato dall’IEEE, ottima indicizzazione in Google Scholar per il settore.</td> </tr> <tr> <td style="text-align: left"><strong>Priorità: flessibilità, DOI standard, archivio dati/codice</strong></td> <td style="text-align: left"><strong>Zenodo</strong></td> <td style="text-align: left">Semplice da usare e garantisce un DOI per ogni tipo di output.</td> </tr> <tr> <td style="text-align: left"><strong>Priorità: massima diffusione nella ricerca pura</strong></td> <td style="text-align: left"><strong>arXiv</strong></td> <td style="text-align: left">Piattaforma leader per i preprint in computer science.</td> </tr> </tbody> </table> <p>Scegliendo una di queste piattaforme, ti assicuri che le tue dispense non siano solo lette, ma diventino parte della letteratura citabile della comunità.</p>]]></content><author><name></name></author><category term="research"/><category term="preprint"/><category term="openaccess"/><summary type="html"><![CDATA[piattaforma online dove pubblicare tutorial, dispense e risultati di ricerca]]></summary></entry><entry><title type="html">Hosting a Personal Website on GitHub Pages</title><link href="https://emanubuc.github.io/blog/2025/hosting-website-github/" rel="alternate" type="text/html" title="Hosting a Personal Website on GitHub Pages"/><published>2025-03-26T14:24:00+00:00</published><updated>2025-03-26T14:24:00+00:00</updated><id>https://emanubuc.github.io/blog/2025/hosting-website-github</id><content type="html" xml:base="https://emanubuc.github.io/blog/2025/hosting-website-github/"><![CDATA[<p>I recently decided it was time to refresh my personal website. Since I prefer to avoid dealing with virtual servers and hosting maintenance, GitHub Pages has become my go-to solution.</p> <p>Hosting a personal website on GitHub is a powerful way to establish a professional online presence with minimal cost and maintenance. In this post, I’ll walk you through the key concepts, benefits, and a practical workflow for getting started with GitHub Pages and a simple static site.</p> <h2 id="why-use-github-for-your-personal-website">Why Use GitHub for Your Personal Website</h2> <p>GitHub offers a feature called GitHub Pages, which can host static websites directly from a repository. This is ideal for personal portfolios, research pages, and blogs written in Markdown.</p> <p><strong>Key advantages include:</strong></p> <ul> <li><strong>Free</strong> hosting with automatic HTTPS and no bandwidth limits for typical personal use.</li> <li>Tight integration with Git: your site is version-controlled, reviewable, and easy to roll back.</li> <li>Native support for static site generators like Jekyll, making Markdown-based blogs and documentation straightforward to manage.</li> </ul> <h2 id="understanding-github-pages">Understanding GitHub Pages</h2> <p>GitHub Pages serves HTML, CSS, JavaScript, and other static assets from a special branch or repository. For a user site, the repository is typically named <code class="language-plaintext highlighter-rouge">username.github.io</code>, and GitHub automatically builds and publishes it.</p> <p><strong>Important characteristics:</strong></p> <ul> <li>Content is static: no server-side code such as Python, PHP, or Node.js runs on GitHub Pages.</li> <li>Sites can be either user/organization sites (<code class="language-plaintext highlighter-rouge">username.github.io</code>) or project-specific sites under any repository.</li> <li>Configuration is managed via repository settings and simple configuration files, so most changes are handled through Git commits.</li> </ul> <h2 id="choosing-a-site-structure-and-workflow">Choosing a Site Structure and Workflow</h2> <p>Before writing content, decide how you want to manage your site:</p> <ul> <li>For a minimal portfolio, a hand-written HTML page plus a stylesheet may be enough.</li> <li>For content-heavy sites or blogs, using a static site generator like Jekyll simplifies layout reuse, navigation, and Markdown-based authoring.</li> <li>A hybrid approach often works well: a Jekyll site with separate sections for an “About” page, a research or projects portfolio, and a blog.</li> <li>A template-based website can offer more customization options.</li> </ul> <h3 id="1-simple-htmlcssjavascript">1. Simple HTML/CSS/JavaScript</h3> <ul> <li>Create a repository named <code class="language-plaintext highlighter-rouge">yourusername.github.io</code></li> <li>Add <code class="language-plaintext highlighter-rouge">index.html</code>, CSS, and JS files</li> <li>Enable GitHub Pages in Settings</li> <li>Your site goes live at <code class="language-plaintext highlighter-rouge">https://yourusername.github.io</code></li> </ul> <h3 id="2-using-jekyll-githubs-built-in-generator">2. Using Jekyll (GitHub’s Built-in Generator)</h3> <ul> <li>Jekyll is automatically supported by GitHub Pages</li> <li>Supports Markdown-based blog posts and pages</li> <li>Choose from GitHub’s supported themes (Minima, Hacker, etc.)</li> <li>Ideal if you want blogging capabilities</li> </ul> <h3 id="3-using-pre-built-templates">3. Using Pre-built Templates</h3> <ul> <li>Clone a portfolio template repository (like <code class="language-plaintext highlighter-rouge">al-folio</code> or others from GitHub Collections)</li> <li>Customize with your content</li> <li>Deploy with a single push</li> </ul> <h3 id="key-features-across-approaches">Key Features Across Approaches</h3> <ul> <li>✅ Free hosting</li> <li>✅ Free HTTPS</li> <li>✅ Custom domain support (optional)</li> <li>✅ No backend server needed (static sites only)</li> <li>✅ Built-in version control</li> <li>✅ Easy updates via Git</li> </ul> <hr/>]]></content><author><name></name></author><category term="hosting"/><category term="hosting"/><category term="website"/><category term="github"/><summary type="html"><![CDATA[I recently decided to refresh my personal website. Since I prefer to avoid server and hosting maintenance, GitHub Pages is my go-to solution.]]></summary></entry><entry><title type="html">Differential Privacy</title><link href="https://emanubuc.github.io/blog/2021/Differential_Privacy/" rel="alternate" type="text/html" title="Differential Privacy"/><published>2021-04-09T10:05:00+00:00</published><updated>2021-04-09T10:05:00+00:00</updated><id>https://emanubuc.github.io/blog/2021/Differential_Privacy</id><content type="html" xml:base="https://emanubuc.github.io/blog/2021/Differential_Privacy/"><![CDATA[<h1 id="differential-privacy">Differential Privacy</h1> <p>Data scientists have an ethical (and often legal) responsibility to protect sensitive data. Differential privacy is a leading edge approach that enables useful analysis while protecting individually identifiable data values.</p> <p>A machine learning project typically involves an iterative process of data analyses in order to gain insights into the data and determine which variables are most likely to help build predictive models. Analyzing data usually involves aggregative and statistical functions that provide insights into the statistical distribution of variables and the relationships between them. With large volumes of data, the aggregations provide a level of abstraction; but with smaller amounts of data, or with repeated analyses, even aggregated results may reveal details about individual observations.</p> <p>Differential privacy is a technique that is designed to preserve the privacy of individual data points by adding “noise” to the data. The goal is to ensure that enough noise is added to provide privacy for individual values while ensuring that the overall statistical makeup of the data remains consistent, and aggregations produce statistically similar results as when used with the original raw data.</p> <h2 id="smartnoisesdk">SmartNoiseSDK</h2> <p><a href="https://smartnoise.org/">SmartNoise</a> is a toolkit from OpenDP; a joint project between researchers at Microsoft, Harvard University, and other contributors that aims to provide building blocks for using differential privacy in data analysis and machine learning projects.</p> <p><img src="/assets/img/smart-noise-sdk-main-graphic.png" alt="smart noise SDK graphic"/></p> <h3 id="how-to-install">How To Install</h3> <pre><code class="language-Python">!pip install opendp-smartnoise==0.1.3.1
</code></pre> <h3 id="add-statistical-noise">Add Statistical Noise</h3> <p>You can use SmartNoise to create an analysis in which noise is added to the source data. The underlying mathematics of how the noise is added can be quite complex, but SmartNoise takes care of most of the details for you. However, there are a few concepts it’s useful to be aware of.</p> <p><strong>Upper and lower bounds</strong>: Clamping is used to set upper and lower bounds on values for a variable. This is required to ensure that the noise generated by SmartNoise is consistent with the expected distribution of the original data. <strong>Sample size</strong>: To generate consistent differentially private data for some aggregations, SmartNoise needs to know the size of the data sample to be generated. <strong>Epsilon</strong>: Put simplistically, epsilon is a non-negative value that provides an inverse measure of the amount of noise added to the data. A low epsilon results in a dataset with a greater level of privacy, while a high epsilon results in a dataset that is closer to the original data. Generally, you should use epsilon values between 0 and 1. Epsilon is correlated with another value named delta, that indicates the probability that a report generated by an analysis is not fully private.</p> <p>As a rule of thumb, $\epsilon$ should be thought of as a small number, between approximately 1/1000and 1. In each implementation of differential privacy, a value of $\epsilon$ that allows a reasonable compromise between privacy and accuracy should be carefully chosen.</p> <p>see https://privacytools.seas.harvard.edu/files/privacytools/files/pedagogical-document-dp_0.pdf</p>]]></content><author><name></name></author><category term="research"/><category term="privacy"/><category term="ML"/><category term="privacy"/><category term="ML"/><summary type="html"><![CDATA[Differential privacy is a leading edge approach that enables useful analysis while protecting individually identifiable data values]]></summary></entry><entry><title type="html">Azure Machine Learning experiments</title><link href="https://emanubuc.github.io/blog/2021/AzureML_Exeperiments/" rel="alternate" type="text/html" title="Azure Machine Learning experiments"/><published>2021-03-11T10:05:00+00:00</published><updated>2021-03-11T10:05:00+00:00</updated><id>https://emanubuc.github.io/blog/2021/AzureML_Exeperiments</id><content type="html" xml:base="https://emanubuc.github.io/blog/2021/AzureML_Exeperiments/"><![CDATA[<h1 id="azure-machine-learning-experiments">Azure Machine Learning experiments</h1> <p>Like any scientific discipline, data science involves running experiments; typically to explore data or to build and evaluate predictive models. In Azure Machine Learning, an experiment is a named process, usually the running of a script or a pipeline, that can generate metrics and outputs and be tracked in the Azure Machine Learning workspace.</p> <p><img src="/assets/img/azure_ml_03-01-experiment.jpg" alt="AzureML Experiment"/></p> <p>An experiment can be run multiple times, with different data, code, or settings; and Azure Machine Learning tracks each run, enabling you to view run history and compare results for each run.</p> <h2 id="the-experiment-run-context">The Experiment Run Context</h2> <p>When you submit an experiment, you use its run context to initialize and end the experiment run that is tracked in Azure Machine Learning, as shown in the following code sample:</p> <pre><code class="language-Python">from azureml.core import Experiment

# create an experiment variable
experiment = Experiment(workspace = ws, name = "my-experiment")

# start the experiment
run = experiment.start_logging()

# experiment code goes here

# end the experiment
run.complete()
</code></pre> <h2 id="logging-metrics-and-creating-outputs">Logging Metrics and Creating Outputs</h2> <p>Experiments are most useful when they produce metrics and outputs that can be tracked across runs</p> <h3 id="logging-metrics">Logging Metrics</h3> <p>Every experiment generates log files that include the messages that would be written to the terminal during interactive execution. This enables you to use simple print statements to write messages to the log. However, if you want to record named metrics for comparison across runs, you can do so by using the Run object; which provides a range of logging functions specifically for this purpose. These include:</p> <ul> <li>log: Record a single named value.</li> <li>log_list: Record a named list of values.</li> <li>log_row: Record a row with multiple columns.</li> <li>log_table: Record a dictionary as a table.</li> <li>log_image: Record an image file or a plot.</li> </ul> <p>For example, following code records the number of observations (records) in a CSV file:</p> <pre><code class="language-Python">from azureml.core import Experiment
import pandas as pd

# Create an Azure ML experiment in your workspace
experiment = Experiment(workspace = ws, name = 'my-experiment')

# Start logging data from the experiment
run = experiment.start_logging()

# load the dataset and count the rows
data = pd.read_csv('data.csv')
row_count = (len(data))

# Log the row count
run.log('observations', row_count)

# Complete the experiment
run.complete()

</code></pre>]]></content><author><name></name></author><category term="research"/><category term="Azure"/><category term="AI"/><category term="ML"/><category term="Azure"/><category term="ML"/><category term="MLOPs"/><summary type="html"><![CDATA[In Azure Machine Learning, an experiment is a named process, usually the running of a script or a pipeline, that can generate metrics and outputs and be tracked in the Azure Machine Learning workspace]]></summary></entry><entry><title type="html">Azure Machine Learning</title><link href="https://emanubuc.github.io/blog/2021/Azure_Machine_Learning/" rel="alternate" type="text/html" title="Azure Machine Learning"/><published>2021-03-10T11:15:00+00:00</published><updated>2021-03-10T11:15:00+00:00</updated><id>https://emanubuc.github.io/blog/2021/Azure_Machine_Learning</id><content type="html" xml:base="https://emanubuc.github.io/blog/2021/Azure_Machine_Learning/"><![CDATA[<h1 id="azure-machine-learning">Azure Machine Learning</h1> <p>Azure Machine Learning is a cloud-based platform for building and operating machine learning solutions in Azure. It includes a wide range of features and capabilities that help data scientists prepare data, train models, publish predictive services, and monitor their usage. Most importantly, it helps data scientists increase their efficiency by automating many of the time-consuming tasks associated with training models; and it enables them to use cloud-based compute resources that scale effectively to handle large volumes of data while incurring costs only when actually used.</p> <p><img src="/assets/img/01-01-what-is-azure-ml.jpg" alt="azure ml"/></p> <p>Built on the Microsoft Azure cloud platform, Azure Machine Learning enables you to manage:</p> <p>Scalable on-demand compute for machine learning workloads. Data storage and connectivity to ingest data from a wide range sources. Machine learning workflow orchestration to automate model training, deployment, and management processes. Model registration and management, so you can track multiple versions of models and the data on which they were trained. Metrics and monitoring for training experiments, datasets, and published services. Model deployment for real-time and batch inferencing.</p> <h2 id="azure-machine-learning-workspace">Azure MAchine Learning Workspace</h2> <p>To use Azure Machine Learning, you create a workspace in your Azure subscription. You can then use this workspace to manage data, compute resources, code, models, and other artifacts related to your machine learning workloads.</p> <p>A workspace is a context for the experiments, data, compute targets, and other assets associated with a machine learning workload.</p> <p>The assets in a workspace include:</p> <ul> <li>Compute targets for development, training, and deployment.</li> <li>Data for experimentation and model training.</li> <li>Notebooks containing shared code and documentation.</li> <li>Experiments, including run history with logged metrics and outputs.</li> <li>Pipelines that define orchestrated multi-step processes.</li> <li>Models that you have trained.</li> </ul> <h3 id="workspaces-as-azure-resources">Workspaces as Azure Resources</h3> <p>Workspaces are Azure resources, and as such they are defined within a resource group in an Azure subscription, along with other related Azure resources that are required to support the workspace.</p> <p><img src="/assets/img/azure_ml_01-02-workspace.png" alt="workspace"/></p> <p>The Azure resources created alongside a workspace include:</p> <ul> <li>A storage account - used to store files used by the workspace as well as data for experiments and model training.</li> <li>An Application Insights instance, used to monitor predictive services in the workspace.</li> <li>An Azure Key Vault instance, used to manage secrets such as authentication keys and credentials used by the workspace.</li> <li>A container registry, created as-needed to manage containers for deployed models.</li> </ul> <p>For each one of the resources listed above there are associated costs even is the workspace in not used. So you must delete the worspace id you do not want to consume your credits1.</p> <h2 id="compute-resources">Compute Resources</h2> <p>There are four kinds of compute resource you can create:</p> <ul> <li>Compute Instances: Development workstations that data scientists can use to work with data and models.</li> <li>Compute Clusters: Scalable clusters of virtual machines for on-demand processing of experiment code.</li> <li>Inference Clusters: Deployment targets for predictive services that use your trained models.</li> <li>Attached Compute: Links to existing Azure compute resources, such as Virtual Machines or Azure Databricks clusters.</li> </ul> <h3 id="creating-a-workspace">Creating a Workspace</h3> <p>You can create a workspace in any of the following ways:</p> <ol> <li>In the Microsoft Azure portal, create a new Machine Learning resource, specifying the subscription, resource group and workspace name.</li> <li>Use the Azure Machine Learning Python SDK to run code that creates a workspace.</li> </ol> <p>For example, the following code creates a workspace named aml-workspace (assuming the Azure ML SDK for Python is installed and a valid subscription ID is specified):</p> <pre><code class="language-Python">from azureml.core import Workspace
    
    ws = Workspace.create(name='aml-workspace', 
                      subscription_id='123456-abc-123...',
                      resource_group='aml-resources',
                      create_resource_group=True,
                      location='eastus'
                     )
</code></pre> <ol> <li>Use the Azure Command Line Interface (CLI) with the Azure Machine Learning CLI extension.</li> </ol> <p>For example, you could use the following command (which assumes a resource group named aml-resources has already been created):</p> <pre><code class="language-Python">az ml workspace create -w 'aml-workspace' -g 'aml-resources'
</code></pre> <h2 id="azure-machine-learning-studio">Azure Machine Learning studio</h2> <p>Azure Machine Learning studio is a web-based tool for managing an Azure Machine Learning workspace.</p> <p><img src="/assets/img/azure_ml-01-03-aml-studio.jpg" alt="Azure Machine Learnign Studio"/></p> <p>To use Azure Machine Learning studio, use a a web browser to navigate to <a href="https://ml.azure.com">https://ml.azure.com</a> and sign in using credentials associated with your Azure subscription. You can then select the subscription and workspace you want to manage.</p> <p>A previously released tool named Azure Machine Learning Studio provided a free service for drag and drop machine learning model development. The studio interface for the Azure Machine Learning service includes this capability in the designer tool, as well as other workspace asset management capabilities. The old tool still availale as “Azure Machine Learning Studio <strong>Classic</strong>”.</p> <p>Azure Machine Learning Studio Classic miss many features present in Azure Machine Learning Worspace, but there is 100% free service plan available.</p> <h2 id="the-azure-machine-learning-sdk">The Azure Machine Learning SDK</h2> <p>While graphical interfaces like Azure Machine Learning studio make it easy to create and manage machine learning assets, it is often advantageous to use a code-based approach to managing resources.</p> <p>Azure Machine Learning provides software development kits (SDKs) for Python and R, which you can use to create, manage, and use assets in an Azure Machine Learning workspace.</p> <h3 id="installing-the-azure-machine-learning-sdk-for-python">Installing the Azure Machine Learning SDK for Python</h3> <p>You can install the Azure Machine Learning SDK for Python by using the pip package management utility.</p> <pre><code class="language-Python">pip install azureml-sdk[notebooks,automl,explain]
</code></pre> <p>For more information about installing the Azure Machine Learning SDK for Python, see the <a href="https://aka.ms/AA70rq7">SDK documentation</a>.</p> <h3 id="retrieving-and-viewing-logged-metrics">Retrieving and Viewing Logged Metrics</h3> <p>You can view the metrics logged by an experiment run in Azure Machine Learning studio or by using the RunDetails widget in a notebook, as shown here:</p> <pre><code class="language-Python">from azureml.widgets import RunDetails

RunDetails(run).show()

</code></pre> <pre><code class="language-Python">import json

# Get logged metrics
metrics = run.get_metrics()
print(json.dumps(metrics, indent=2))

</code></pre> <h2 id="experiment-output-files">Experiment Output Files</h2> <p>In addition to logging metrics, an experiment can generate output files. Often these are trained machine learning models, but you can save any sort of file and make it available as an output of your experiment run. The output files of an experiment are saved in its outputs folder.</p> <pre><code class="language-Python">run.upload_file(name='outputs/sample.csv', path_or_stream='./sample.csv')
</code></pre> <p>When running an experiment in a remote compute context, any files written to the outputs folder in the compute context are automatically uploaded to the run’s outputs folder when the run completes.</p> <p>Whichever approach you use to run your experiment, you can retrieve a list of output files from the Run object like this:</p> <pre><code class="language-Python">import json

files = run.get_file_names()
print(json.dumps(files, indent=2))
</code></pre> <h2 id="running-a-script-as-an-experiment">Running a Script as an Experiment</h2> <p>To run a script as an experiment, you must define a script configuration that defines the script to be run and the Python environment in which to run it. This is implemented by using a ScriptRunConfig object.</p> <p>For example, the following code could be used to run an experiment based on a script in the experiment_files folder</p> <pre><code class="language-Python">from azureml.core import Experiment, ScriptRunConfig

# Create a script config
script_config = ScriptRunConfig(source_directory=experiment_folder,
                                script='experiment.py') 

# submit the experiment
experiment = Experiment(workspace = ws, name = 'my-experiment')
run = experiment.submit(config=script_config)
run.wait_for_completion(show_output=True)

</code></pre> <p>An experiment script (such as experiment.py) is just a Python code file that contains the code you want to run in the experiment. To access the experiment run context (which is needed to log metrics) the script must import the <strong>azureml.core.Run</strong> class and call its get_context method. The script can then use the run context to log metrics, upload files, and complete the experiment.</p> <pre><code class="language-Python">from azureml.core import Run
import pandas as pd
import matplotlib.pyplot as plt
import os

# Get the experiment run context
run = Run.get_context()

# load the diabetes dataset
data = pd.read_csv('data.csv')

# Count the rows and log the result
row_count = (len(data))
run.log('observations', row_count)

# Save a sample of the data
os.makedirs('outputs', exist_ok=True)
data.sample(100).to_csv("outputs/sample.csv", index=False, header=True)

# Complete the run
run.complete()

</code></pre>]]></content><author><name></name></author><category term="research"/><category term="Azure"/><category term="AI"/><category term="ML"/><category term="Azure"/><category term="ML"/><category term="MLOPs"/><summary type="html"><![CDATA[Azure Machine Learning is a cloud-based platform for building and operating machine learning solutions in Azure.]]></summary></entry><entry><title type="html">Azure Machine Learning - Hyperparameter Tuning</title><link href="https://emanubuc.github.io/blog/2021/Hyperparameter-tuning/" rel="alternate" type="text/html" title="Azure Machine Learning - Hyperparameter Tuning"/><published>2021-03-09T09:08:00+00:00</published><updated>2021-03-09T09:08:00+00:00</updated><id>https://emanubuc.github.io/blog/2021/Hyperparameter-tuning</id><content type="html" xml:base="https://emanubuc.github.io/blog/2021/Hyperparameter-tuning/"><![CDATA[<h1 id="azure-machine-learning---hyperparameter-tuning">Azure Machine Learning - Hyperparameter Tuning</h1> <p>In machine learning, models are trained to predict unknown labels for new data based on correlations between known labels and features found in the training data. Depending on the algorithm used, you may need to specify hyperparameters to configure how the model is trained.</p> <p>For example, the logistic regression algorithm uses a regularization rate hyperparameter to counteract overfitting; and deep learning techniques for convolutional neural networks (CNNs) use hyperparameters like learning rate to control how weights are adjusted during training, and batch size to determine how many data items are included in each training batch.</p> <p>Data scientists refer to the values determined from the training features as <em>parameters</em>, <strong>values that are used to configure training behavior but which are not derived from the training data</strong> are named <em>hyperparameter</em>.</p> <p>The choice of hyperparameter values can significantly affect the resulting model, making it important to select the best possible values.</p> <p>Hyperparameter tuning is accomplished by training the multiple models, using the same algorithm and training data but different hyperparameter values. The resulting model from each training run is then evaluated to determine the performance metric for which you want to optimize (for example, <em>accuracy</em>), and the best-performing model is selected.</p> <p>In Azure Machine Learning, you achieve this through an <em>experiment</em> that consists of a <em>hyperdrive run</em>, which initiates a child run for each hyperparameter combination to be tested. Each child run uses a training script with parameterized hyperparameter values to train a model, and logs the target performance metric achieved by the trained model.</p> <p>Hyperdrive run configuration must include:</p> <ul> <li>hyperparameter search space.</li> <li>hyperparameter sampling.</li> <li>early-termination policy.</li> </ul> <h2 id="sampling">Sampling</h2> <h3 id="grid-sampling">Grid Sampling</h3> <p>Grid Sampling is used to try every possible combination of parameters in the search space.</p> <pre><code class="language-Python">from azureml.train.hyperdrive import GridParameterSampling, choice

param_space = {
                 '--batch_size': choice(16, 32, 64),
                 '--learning_rate': choice(0.01, 0.1, 1.0)
              }

param_sampling = GridParameterSampling(param_space)

</code></pre> <h3 id="random-samplig">Random Samplig</h3> <p>Random sampling is used to randomly select a value for each hyperparameter</p> <pre><code class="language-Python">from azureml.train.hyperdrive import RandomParameterSampling, choice, normal

param_space = {
                 '--batch_size': choice(16, 32, 64),
                 '--learning_rate': normal(10, 3)
              }

param_sampling = RandomParameterSampling(param_space)

</code></pre> <h3 id="bayesian-sampling">Bayesian sampling</h3> <p>Bayesian sampling chooses hyperparameter values based on the Bayesian optimization algorithm, which tries to select parameter combinations that will result in improved performance from the previous selection.</p> <pre><code class="language-Python">from azureml.train.hyperdrive import BayesianParameterSampling, choice, uniform

param_space = {
                 '--batch_size': choice(16, 32, 64),
                 '--learning_rate': uniform(0.05, 0.1)
              }

param_sampling = BayesianParameterSampling(param_space)
</code></pre> <p>Note: Bayesian sampling can be used only with choice, uniform, and quniform parameter expressions, and can not be combined with an early-termination policy</p> <h2 id="early-termination-policy">Early Termination Policy</h2> <p>With a sufficiently large hyperparameter search space, it could take many iterations (i.e. child runs) to try every possible combination. To prevent wasting time and money, in addition to a maximum number of rins, you can set an <strong>early termination policy</strong> that abandons runs that are unlikely to produce a better result than previously completed runs.</p> <p>The policy is evaluated at an <code class="language-plaintext highlighter-rouge">evaluation_interval</code> you specify. You can also set a <code class="language-plaintext highlighter-rouge">delay_evaluation</code> parameter to avoid evaluating the policy until a minimum number of iterations have been completed.</p> <p>Early termination is particularly useful for deep learning scenarios where a deep neural network (DNN) is trained iteratively over a number of epochs. The training script can report the target metric after each epoch, and if the run is significantly underperforming previous runs after the same number of intervals, it can be abandoned.</p> <h3 id="bandit-policy">Bandit Policy</h3> <p>Use a bandit policy to stop a run if the target performance metric underperforms the best run so far by a specified margin expressed as absolute value (<code class="language-plaintext highlighter-rouge">slack_amount</code>) or factor (<code class="language-plaintext highlighter-rouge">slack_factor</code>)</p> <p>This example applies the policy for every iteration after the first five, and abandons runs where the reported target metric is 0.2 or more worse than the best performing run after the same number of intervals.</p> <pre><code class="language-Python">from azureml.train.hyperdrive import BanditPolicy

early_termination_policy = BanditPolicy(slack_amount = 0.2,
                                        evaluation_interval=1,
                                        delay_evaluation=5)
</code></pre> <p>You can also apply a bandit policy using a slack factor, which compares the performance metric as a ratio rather than an absolute value.</p> <h3 id="median-stopping-policy">Median Stopping Policy</h3> <p>A median stopping policy abandons runs where the target performance metric is worse than the median of the running averages for all runs.</p> <pre><code class="language-Python">from azureml.train.hyperdrive import MedianStoppingPolicy

early_termination_policy = MedianStoppingPolicy(evaluation_interval=1,
                                                delay_evaluation=5)
</code></pre> <h3 id="truncation-selection-policy">Truncation selection policy</h3> <p>A truncation selection policy cancels the lowest performing X% of runs at each evaluation interval based on the truncation_percentage value you specify for X.</p> <pre><code class="language-Python">from azureml.train.hyperdrive import TruncationSelectionPolicy

early_termination_policy = TruncationSelectionPolicy(truncation_percentage=10,
                                                     evaluation_interval=1,
                                                     delay_evaluation=5)

</code></pre> <h2 id="running-a-hyperparameter-tuning-experiment">Running a hyperparameter tuning experiment</h2> <p>In Azure Machine Learning, you can tune hyperparameters by running a hyperdrive experiment.</p> <h3 id="training-script-for-hyperparameter-tuning">Training script for hyperparameter tuning</h3> <p>To run a hyperdrive experiment, you need to create a training script just the way you would do for any other training experiment, except that your script must:</p> <ul> <li>Include an argument for each hyperparameter you want to vary.</li> <li>Log the target performance metric. This enables the hyperdrive run to evaluate the performance of the child runs it initiates, and identify the one that produces the best performing model.</li> </ul> <p>You can found an example of HyperDrive experiment and a <a href="https://github.com/emanbuc/Optimizing_a_Pipeline_in_AzureML/blob/master/train.py">training script</a> in the “Optimizing a Pipaline in AzureML” <a href="https://github.com/emanbuc/Optimizing_a_Pipeline_in_AzureML">GitHub repository</a></p> <p>A reference training script example you can use as template is the following one:</p> <pre><code class="language-Python"># using a --regularization argument to set the regularization rate 
# hyperparameter, and logs the accuracy metric with the name Accuracy

import argparse
import joblib
from azureml.core import Run
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

# Get regularization hyperparameter
parser = argparse.ArgumentParser()
parser.add_argument('--regularization', type=float, dest='reg_rate', default=0.01)
args = parser.parse_args()
reg = args.reg_rate

# Get the experiment run context
run = Run.get_context()

# load the training dataset
data = run.input_datasets['training_data'].to_pandas_dataframe()

# Separate features and labels, and split for training/validatiom
X = data[['feature1','feature2','feature3','feature4']].values
y = data['label'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)

# Train a logistic regression model with the reg hyperparameter
model = LogisticRegression(C=1/reg, solver="liblinear").fit(X_train, y_train)

# calculate and log accuracy
y_hat = model.predict(X_test)
acc = np.average(y_hat == y_test)
run.log('Accuracy', np.float(acc))

# Save the trained model
os.makedirs('outputs', exist_ok=True)
joblib.dump(value=model, filename='outputs/model.pkl')

run.complete()

</code></pre> <h3 id="configuring-hyperdrive-experiment">Configuring hyperdrive experiment</h3> <p>To configure HyperDrive you must use HyperDriveConfig class as shown in the following example:</p> <p>from azureml.core import Experiment from azureml.train.hyperdrive import HyperDriveConfig, PrimaryMetricGoal</p> <pre><code class="language-Python">
# Assumes ws, script_config and param_sampling are already defined
hyperdrive = HyperDriveConfig(run_config=script_config,
                              hyperparameter_sampling=param_sampling,
                              policy=None,
                              primary_metric_name='Accuracy',
                              primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,
                              max_total_runs=6,
                              max_concurrent_runs=4)

experiment = Experiment(workspace = ws, name = 'hyperdrive_training')
hyperdrive_run = experiment.submit(config=hyperdrive)
</code></pre> <h3 id="monitoring-and-reviewing-hyperdrive-runs">Monitoring and reviewing hyperdrive runs</h3> <p>You can monitor hyperdrive experiments in Azure Machine Learning studio, or by using the Jupyter Notebooks RunDetails widget.</p> <p><img src="/assets/img/hyperdrive_monitoring-experiment.png" alt="monitoring experiment"/></p>]]></content><author><name></name></author><category term="research"/><category term="Azure"/><category term="AI"/><category term="ML"/><category term="Azure"/><category term="ML"/><category term="MLOPs"/><summary type="html"><![CDATA[The choice of hyperparameter values can significantly affect the resulting model, making it important to select the best possible values.]]></summary></entry><entry><title type="html">Machine Learning and Big Data Course Outline</title><link href="https://emanubuc.github.io/blog/2021/Machine_Learning_And_Big_Data/" rel="alternate" type="text/html" title="Machine Learning and Big Data Course Outline"/><published>2021-03-08T10:08:00+00:00</published><updated>2021-03-08T10:08:00+00:00</updated><id>https://emanubuc.github.io/blog/2021/Machine_Learning_And_Big_Data</id><content type="html" xml:base="https://emanubuc.github.io/blog/2021/Machine_Learning_And_Big_Data/"><![CDATA[<h1 id="machine-learning-and-big-data">Machine Learning and Big Data</h1> <h2 id="table-of-content">Table of Content</h2> <ul> <li><a href="2021-03-07-big_data_meet_ml.html">Big Data Meet ML</a> <ul> <li>What is ML</li> <li>What is Big Data</li> <li>For Big Data Analysis we need Machine Learning</li> <li>For ML we need Big Data</li> <li>ML for data analysis in physics and other sciences</li> </ul> </li> <li> <p><a href="2021-03-05-AI_at_scale.html">AI at Scale</a></p> </li> <li>ML on Big Data <ul> <li>Big Data -&gt; Big Opportunities</li> <li>Big Data -&gt; Big Challenges</li> <li>Survey of Machine Learning for Big DataProcessing</li> </ul> </li> <li>Distributed ML Training <ul> <li>Tools and Techniques for distributed Training</li> <li>Data parallelism / Model parallelism</li> </ul> </li> <li>Compute resources for ML <ul> <li>compute hardware for ML</li> <li>local vs cloud</li> <li>Why cloud</li> <li>When (not) cloud</li> <li>Cloud platform for ML</li> <li>Notebook server ad Full server solution</li> <li>Notebook: Colab, Gradient</li> <li>Full serer: Google Cloud, Azure</li> </ul> </li> <li>A Close look at Azure Machine Learning <ul> <li>Azure for student (with @unipg.it account)</li> <li>Azure ML Platform</li> <li>Azure ML Studio</li> <li>Azure ML SDK</li> <li>Training Workflow</li> <li>ML Pipeline / ML OPS</li> <li>Deploy</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="Courses"/><category term="AI"/><category term="ML"/><category term="BigData"/><category term="AI"/><category term="ML"/><category term="BigData"/><summary type="html"><![CDATA[Machine Learning and Big Data lecture topics]]></summary></entry><entry><title type="html">Machine Learning (ML) Meets Big Data</title><link href="https://emanubuc.github.io/blog/2021/big_data_meet_ml/" rel="alternate" type="text/html" title="Machine Learning (ML) Meets Big Data"/><published>2021-03-07T10:08:00+00:00</published><updated>2021-03-07T10:08:00+00:00</updated><id>https://emanubuc.github.io/blog/2021/big_data_meet_ml</id><content type="html" xml:base="https://emanubuc.github.io/blog/2021/big_data_meet_ml/"><![CDATA[<h1 id="machine-learning-ml-meets-big-data">Machine Learning (ML) Meets Big Data</h1> <p>In the past few years, more data has been produced than whole human history before. This data represents a gold mine in terms of commercial value and also important reference material for policy makers. But much of this value will stay unexploited until the tools for processing the huge amount of information remain unavailable</p> <h2 id="what-is-machine-learning">What is Machine Learning?</h2> <p>The core of machine learning consists of self-learning algorithms that evolve by continuously improving at their assigned task. Algorithms fine-tune themselves with the data they train on. They build their own logic and, as a result, create solutions relevant to aspects of our world as diverse as fraud detection, web searches, tumor classification, object recognition and price prediction.</p> <h2 id="what-is-big-data">What is Big Data?</h2> <p>Data consists of numbers, words, measurements and observations formatted in ways computers can process. The digital era presents a challenge for traditional data-processing software: information becomes available in such volume, velocity and variety that it ends up outpacing human-centered computation. And we can describe big data using these three “V”s: volume, velocity and variety. </p> <ul> <li>Volume refers to the scale of available data;</li> <li>velocity is the speed with which data is accumulated;</li> <li>variety refers to the different sources it comes from.</li> </ul> <p>Two other Vs are often added to previous three:</p> <ul> <li>Veracity refers to the consistency</li> <li>certainty (or lack thereof) in the sourced data, while value measures the usefulness of the data that’s been extracted from the data received. </li> </ul> <h2 id="big-data-meet-big-data">Big Data Meet Big Data</h2> <p>Machine-learning algorithms become more effective as the size of training datasets grows. So when combining big data with machine learning, we benefit twice:</p> <ol> <li>the algorithms help us keep up with the continuous influx of data</li> <li>the volume and variety of the same data feeds the algorithms and helps them to achive better performance</li> </ol> <h3 id="value-of-data">Value of Data</h3> <p>TBC</p> <h3 id="for-big-data-analysis-we-need-machine-learning">For Big Data Analysis we need Machine Learning</h3> <p>more than one hour of video is uploaded to YouTube every second, amounting to 10 years of content every day; the genomes of 1000s of people, each of which has a length of more than a billion base pairs, have been sequenced by various labs and so on. This deluge of data calls for automated methods of data analysis, which is exactly what machine learning provides.</p> <h3 id="ml-for-data-analysis-in-physics-and-other-sciences">ML for data analysis in physics and other sciences</h3> <p>Some exmple form <a href="https://phys.org/">Phys.org</a></p> <p><img src="/assets/img/phy0.png" alt="Post from Phys.org"/></p> <p><img src="/assets/img/phy01.png" alt="Post from Phys.org"/></p> <p><img src="/assets/img/phy02.png" alt="Post from Phys.org"/></p> <p><img src="/assets/img/phy03.png" alt="Post from Phys.org"/></p> <p><img src="/assets/img/phy04.png" alt="Post from Phys.org"/></p> <h2 id="for-ml-we-need-big-data">For ML we need Big Data</h2> <p>TBC</p>]]></content><author><name></name></author><category term="research"/><category term="AI"/><category term="ML"/><category term="BigData"/><category term="AI"/><category term="ML"/><category term="BigData"/><summary type="html"><![CDATA[data represents a gold mine in terms of commercial value and also important reference material for policy makers. But much of this value will stay unexploited until the tools for processing the huge amount of information remain unavailable]]></summary></entry><entry><title type="html">Migrate an existing SQL database from local server to Azure SQL</title><link href="https://emanubuc.github.io/blog/2021/create-azure-sql-db-from-data-export/" rel="alternate" type="text/html" title="Migrate an existing SQL database from local server to Azure SQL"/><published>2021-03-05T12:10:00+00:00</published><updated>2021-03-05T12:10:00+00:00</updated><id>https://emanubuc.github.io/blog/2021/create-azure-sql-db-from-data-export</id><content type="html" xml:base="https://emanubuc.github.io/blog/2021/create-azure-sql-db-from-data-export/"><![CDATA[<h1 id="create-a-new-azure-sql-database-from-data-tier-application-export">Create a New Azure SQL Database from Data Tier Application Export</h1> <p>If you want to migrate an existing SQL database from local server to Azure SQL the old style backup and restore with *.back file can not be used with Azure SQL managed database. Also the newer Import/Export Data Tier application may fails depending on the PK /FK present into the tables.</p> <p>There are specific tools you can use to migrate an existing database to Azure SQL.</p> <ul> <li>“Deploy to Azure” .</li> <li>“Import Database”</li> <li>Data Migration Assistant (DMA)</li> </ul> <p>The first step is in both cases to create a backpack file with “Export Data Tier Application”</p> <p><img src="/assets/img/sql_export_data_tier_01.png" alt="export 1"/></p> <p><img src="/assets/img/sql_export_data_tier_01.png" alt="export 1"/></p> <h2 id="deploy-database-to-microsoft--azure-sql">Deploy database to Microsoft Azure SQL</h2> <p><em>If you can not connect to Azure from the original server you may restore a copy of the original DB in PC connected to Internet and the from here run “Deploy Database to Azure SQL”.</em></p> <p><img src="/assets/img/creazione_db_azure_sql_04.png" alt="Deploy to azure"/></p> <p>Follow the wizard and fill all required field</p> <p><img src="/assets/img/creazione_db_azure_sql_05.png" alt="Deployment settings "/></p> <p>You need to connect to Azure SQL server with a user with privileges to create a new database such as a user with role “contributor” assigned in RBAC.</p> <p><img src="/assets/img/creazione_db_azure_sql_01.png" alt="Login to SQL server"/></p> <p>Please nota that you must select the appropriate authentication type from the drop-down list. Multi Factor Authentication (MFA) is active for may account so “Azure Active Directory” is the correct one from me.</p> <p>Then the appropriate service plan for the new database. Nota that these settings can be changed later after deploy completed.</p> <p><img src="/assets/img/creazione_db_azure_sql_06.png" alt="Deployment settings - part 2 "/></p> <p>Review deploy setting and click “Finish”. When you do, you will see the progress of the migration, step by step while waiting until completed.</p> <p><img src="/assets/img/creazione_db_azure_sql_07.png" alt="Deployment settings - review "/></p> <p>As you can see above, there are number of steps. You can also see that there is a step named “Processing Table”. This step will show each table as it is processed and migrated to Azure SQL Database.</p> <p><img src="/assets/img/creazione_db_azure_sql_08.png" alt="Deploying "/></p> <h2 id="import-database">Import Database</h2> <p>TBC</p> <p><img src="/assets/img/azure_sql_import_01.png" alt="import database"/></p> <h2 id="data-migration-assistant">Data Migration Assistant</h2> <p>This is a relatively easy tool to use. It really has two purposes.</p> <ol> <li>identify what challenges you might encounter when migrating at database</li> <li>actually complete the migration.</li> </ol> <p>This is a tool the must be <a href="https://www.microsoft.com/en-us/download/details.aspx?id=53595">downloaded</a> and installed before using. Once installed you will find it as <em>“Microsoft Data Migration Assistant”</em> <strong>in the Start menu, not under the SQL Server items</strong>.</p>]]></content><author><name></name></author><category term="Azure"/><category term="DB"/><category term="SQL"/><category term="Azure"/><category term="DB"/><summary type="html"><![CDATA[There are specific tools you can use to migrate an existing database to Azure SQL.]]></summary></entry></feed>