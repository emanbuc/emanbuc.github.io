<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Eseguire un assistente AI basato su LLM in locale | Emanuele Buchicchio </title> <meta name="author" content="Emanuele Buchicchio"> <meta name="description" content="L'esecuzione di modelli di linguaggio di grandi dimensioni (LLM) in locale è diventata oggi una realtà pratica per utenti privati grazie alla maturazione di software intuitivi e a tecniche di compressione avanzate"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://emanubuc.github.io/blog/2026/esecuzione-locale-llm/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Emanuele</span> Buchicchio </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Eseguire un assistente AI basato su LLM in locale</h1> <p class="post-meta"> Created on January 01, 2026 </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> LLM</a>   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   ·   <a href="/blog/category/ai"> <i class="fa-solid fa-tag fa-sm"></i> AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>L’esecuzione di modelli di linguaggio di grandi dimensioni (LLM) in locale è diventata oggi una realtà pratica per utenti privati grazie alla maturazione di software intuitivi e a tecniche di compressione avanzate. Gestire un’IA sul proprio PC o notebook offre vantaggi determinanti come la privacy assoluta dei dati, l’accesso offline, l’assenza di costi di abbonamento e la possibilità di personalizzazione avanzata.</p> <p>Questa transizione è stata resa possibile dalla convergenza di tre fattori critici: la disponibilità di modelli open-weight di classe enterprise, come le serie Llama, Mistral e Qwen; lo sviluppo di tecniche di compressione sofisticate note come quantizzazione; e la democratizzazione dell’hardware ad alte prestazioni, incluse le GPU consumer e i chip con memoria unificata.</p> <p>L’esecuzione locale di LLM è diventata un’imperativo strategico per le organizzazioni che intendono mantenere la sovranità sui propri dati, ottimizzare i costi operativi e garantire la continuità del servizio in assenza di connettività internet. Per supportare carichi di lavoro elevati in contesti aziendali (molti utenti e numero richieste di inferenza anche contemporanee) è ncessario disporre di compenti hardware specifiche, dove la larghezza di banda della memoria e la VRAM della GPU risultano cruciali per garantire prestazioni accettabili. In questo articolo ci occupermo però solo dello scenario di utilizzo “personale” (singolo utente) e per un utilizzo personale un notebook recente con 16/32GB di RAM o un Mac Mini possono esserre già sufficienti.</p> <h2 id="1-installazione-ed-esecuzione-in-locale-di-un-llm">1. Installazione ed Esecuzione in Locale di un LLM</h2> <p>I modelli LLM non sono eseguibili autonomi ma vengono gestiti da un motore di inferenza. Tipicamente i modelli sono sviluppati con framework come TensorFlow, PyTorch o altri, che includono anche gli strumenti per il caricamento di un modello pre-addestrato e permettono di utilizzarlo per per operazioni di inferenze. Il motore di esecuzione gestisce l’allocazione della memoria, e le operazioni di calcolo distribuito su CPU o GPU. Per utilizzare un LLM (ma anche un altro qualsiasi altro modello di machine learning) in genere servono quattro elementi:</p> <ol> <li>Un modello (cioè la struttura del modello)</li> <li>I “pesi” generati dal processo di addestramento. In pratica si tratta dei valori numerici che alla fine del processo di addestramento sono stati assegnati ai parametri del modello. Un modello (architettura) dotato di 70B (Bilions= migliardi) di parametri prevede 70B di “pesi” (valori da assegnare ai suoi parametri). Quando si <em>scarica</em> un modello in pratica si scarica il file con pesi (valori) da assegnare a tutti i parametri oltre che la struttura del modello con i parametri non valorizzati.</li> <li>Un motore di inferenza. A volte può essere lo stesso usato per l’addestramento, ma può essere anche diverso e specializzato solo per l’inferenza come ad esempio Llama.cpp</li> <li>Hardware con risorse di calcolo sufficienti per il caricamento e l’esecuzione del modello</li> </ol> <p>Ad esempio è possibile eseguire modelli pre-addestrati usanto da Libreria (<a href="https://huggingface.co/docs/transformers/model_doc/llama3" rel="external nofollow noopener" target="_blank">Transfromer </a>) di HugginFace in Python</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">transformers</span>
<span class="kn">import</span> <span class="n">torch</span>

<span class="n">model_id</span> <span class="o">=</span> <span class="sh">"</span><span class="s">meta-llama/Meta-Llama-3-8B</span><span class="sh">"</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">transformers</span><span class="p">.</span><span class="nf">pipeline</span><span class="p">(</span><span class="sh">"</span><span class="s">text-generation</span><span class="sh">"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_id</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">torch_dtype</span><span class="sh">"</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">},</span> <span class="n">device_map</span><span class="o">=</span><span class="sh">"</span><span class="s">auto</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">pipeline</span><span class="p">(</span><span class="sh">"</span><span class="s">Hey how are you doing today?</span><span class="sh">"</span><span class="p">)</span>

</code></pre></div></div> <p>Per un utilizzo al difuori dell’ambiente di sviluppo è solitamente preferibile utilizzare utilizzare dei motori di inferenza dedicati che permettono una maggiore efficienza nell’utilizzo delle risosrse di calcolo.</p> <p>I modelli (architettura e pesi) sono disponibili in vari formati. Negli ultimi anni si è affermato il formato <a href="https://huggingface.co/docs/hub/gguf" rel="external nofollow noopener" target="_blank">GUFF</a> per la distribuzione di modelli complessi. La cosa importante è che il formato sia compatibile con il modete di inferenza utilizzato!</p> <p>I motori di inferemza dispongono solitamente di una libreria di modelli già preparata nel formato ottimale per lo specifico motore. La confersione è possibile, ma si tratta di un processo che può nascondere delle insidie e sicuramente da evitare durante i primi esperimenti con gli LLM.</p> <h2 id="2-requisiti-hardware">2. Requisiti Hardware</h2> <p>Sebbene gli LLM siano nati per girare su server potenti, l’hardware consumer attuale (2025/2026) può gestire modelli di dimensioni contenute con prestazioni soddisfacenti.</p> <ul> <li> <strong>Memoria RAM/VRAM:</strong> È il fattore più critico. Se il modello risiede interamente nella memoria video (<strong>VRAM</strong>) della scheda grafica (GPU), l’esecuzione è da 5 a 20 volte più rapida rispetto alla RAM di sistema. <ul> <li> <strong>Configurazione Base (8GB RAM):</strong> Può far girare piccoli modelli da 1B-3B parametri.</li> <li> <strong>Configurazione Consigliata (16GB+ RAM):</strong> Il punto di equilibrio per modelli da <strong>7B-8B</strong> parametri, come Llama 3 o Mistral.</li> <li> <strong>Apple Silicon (M1/M2/M3/M4):</strong> Questi chip offrono un vantaggio unico grazie alla <strong>Memoria Unificata</strong>, che permette alla GPU di accedere a tutta la RAM di sistema come se fosse VRAM.</li> </ul> </li> <li> <strong>Scheda Video (GPU):</strong> Le schede <strong>NVIDIA RTX</strong> (serie 3000/4000/5000) sono lo standard grazie al supporto CUDA. Una GPU con <strong>8GB-12GB di VRAM</strong> (come la RTX 3060 o 4060) è ideale per un’esperienza fluida con modelli medi.</li> <li> <strong>Processore (CPU):</strong> Se non si dispone di una GPU dedicata, è necessaria una CPU moderna (almeno 11ª gen Intel o AMD Zen 4) con supporto alle istruzioni <strong>AVX-512</strong> per accelerare i calcoli.</li> </ul> <h2 id="3-limportanza-della-quantizzazione">3. L’importanza della Quantizzazione</h2> <p>Senza compressione, un modello da 7 miliardi di parametri richiederebbe circa 14-16 GB di VRAM, risultando inaccessibile per hardware standard. La <strong>quantizzazione</strong> riduce la precisione numerica dei pesi del modello (es. da 16-bit a 4-bit) senza comprometterne drasticamente l’intelligenza.</p> <p>Il formato <strong>Q4_K_M</strong> (4-bit) è considerato il <strong>“gold standard”</strong> per l’uso locale: riduce le dimensioni del modello del 75%, permettendo a un modello 7B di girare su soli 5-6 GB di VRAM con una perdita di qualità trascurabile.</p> <p>Il tema dei formati dei modelli e della quantizzazione è ampio e lo approfondiremo in un altra occasione.</p> <h2 id="4-strumenti-software-principali">4. Strumenti Software Principali</h2> <p>Esistono diverse piattaforme che riducono il processo di installazione e configuraizone a pochi click. La piattaforma di esecuzione più performante per un utilizzo personale è sicuramente <a href="https://github.com/ggml-org/llama.cpp" rel="external nofollow noopener" target="_blank">Llama.cpp</a>.</p> <p>Llama.cpp è un’implementazione in puro C/C++ creata da Georgi Gerganov. La sua importanza risiede nel fatto che è stata la prima a dimostrare che i modelli linguistici di grandi dimensioni potevano girare in modo efficiente su hardware consumer (sia CPU che GPU) grazie a tecniche avanzate di quantizzazione e gestione della memoria. Supporta una vasta gamma di accelerazioni, tra cui CUDA (NVIDIA), Metal (Apple Silicon), ROCm (AMD) e Vulkan.</p> <p>Llama.cpp dispone di una sua interfaccia a riga di comando, di una interfaccia web in stile ChatGPT e di un server che ne permette l’utilizzo anche da parte di altri client.</p> <p>Ad esempio</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>llama-server -hf ggml-org/gpt-oss-20b-GGUF --n-cpu-moe 12 -c 32768 --jinja --no-mmap
</code></pre></div></div> <p>Avvia carica il modello GPT-OSS 20B ed avvia il server e l’interfaccia web.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/llama-gpt-oss-480.webp 480w,/assets/img/llama-gpt-oss-800.webp 800w,/assets/img/llama-gpt-oss-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/llama-gpt-oss.png" width="100%" height="auto" title="Optional hover title" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 1: llama-server in esecuzione.</figcaption> </figure> <p>Ovviamene il “peso” dell’esecuzione del modello si fa sentire e non permette di eseguire contemporanemante altre attività pesanti però il sistema è utilizzabile.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gpt-oss-web-interface-480.webp 480w,/assets/img/gpt-oss-web-interface-800.webp 800w,/assets/img/gpt-oss-web-interface-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/gpt-oss-web-interface.png" width="100%" height="auto" title="Optional hover title" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 2: Interfaccia web in stile Chat esposta da llama-server.</figcaption> </figure> <p>Altri strumenti molto popolari, che aggiungono un interfaccia utente più elaborata sopra Llama.cpp sacrificando però le prestazioni compessive: la differenza tra Llama.cpp ed altri strumenti come Ollama è in alcuni casi è piuttosto evidente con Llama.cpp che è arriva ad essere anche un 30-40% più veloce degli altri strumenti. Ad esempio, sul mio Mac Mini M2 16GB con Llama.cpp il modello GPT-OSS 20B è utilizzabile, mentre con altri strumenti riesce a malapena ad avviarsi e poi è troppo lento per un qualsiasi utilizzo pratico interattivo.</p> <p>Altri strumenti popolari per eseguire LLM in locale sono:</p> <ul> <li> <strong>LM Studio:</strong> Forse la soluzione più accessibile per utenti non tecnici. Offre un’interfaccia grafica completa, un motore di ricerca integrato per scaricare modelli da Hugging Face e permette di visualizzare l’uso delle risorse in tempo reale.</li> <li> <strong>Ollama:</strong> Estremamente popolare tra gli utenti più esperti e gli sviluppatori. Funziona tramite riga di comando (CLI) ma può essere abbinato a interfacce web grafiche come <strong>OpenWebUI</strong> per un’esperienza simile a ChatGPT.</li> <li> <strong>Jan.ai:</strong> Un’alternativa <strong>open-source</strong> focalizzata sulla privacy che funziona interamente offline. Ha un’interfaccia minimalista e supporta l’estensione tramite plugin.</li> <li> <strong>GPT4All:</strong> Ottimizzato per girare su hardware modesto (anche solo CPU). Include la funzione <strong>LocalDocs</strong>, che permette di “chattare” con i propri documenti locali (PDF, Word) in modo semplice.</li> </ul> <h2 id="5-modelli-consigliati-per-hardware-standard">5. Modelli Consigliati per Hardware Standard</h2> <p>Per un PC o notebook standard, suggerisco di iniziare con le versioni più piccole e quantizzate di queste famiglie di modelli:</p> <ul> <li> <strong>Llama 3.2 (1B / 3B):</strong> Ultra-leggeri, ideali per notebook con poca RAM.</li> <li> <strong>Mistral (7B) / Llama 3.1 (8B):</strong> I modelli più versatili per compiti generali e scrittura.</li> <li> <strong>Qwen 2.5/3 (0.5B - 7B):</strong> Eccellenti per il coding e compiti multilingue.</li> <li> <strong>Phi-3/4 (Mini):</strong> Modelli piccoli di Microsoft con capacità di ragionamento sorprendenti per la loro dimensione.</li> </ul> <p>Una volta testato il funzionamento di un modello di piccole dimensioni, vi consiglio di testare anche modelli più grandi fino a trovare il compromesso ottimale tra utilizzo delle risorse di calcolo disponibili e prestazioni.</p> <h3 id="openai-gpt-oss">OpenAI GPT-OSS</h3> <p>OpenAI GPT-OSS (Agosto 2025) è il primo rilascio open-weight significativo di OpenAI dopo anni. Il modello è disponibile principalmente nelle versioni 20B e 120B. La variante 120B, in particolare, offre prestazioni di livello GPT-4 ed eccelle in compiti di ragionamento avanzato, tool calling e workflow agentici complessi.</p> <ul> <li> <p>La versione 20B è quella più indicata per hardware locale avanzato, ma richiede comunque una configurazione di fascia alta con almeno 32GB di RAM/VRAM per girare correttamente. <em>Sul mio Mac Mini Apple Silicon M2 con 16GB di RAM condivisa</em> si riesce ad eseguire usando LLAMA-CPP <a href="https://github.com/ggml-org/llama.cpp/discussions/15396" rel="external nofollow noopener" target="_blank">seguendo la guida dedicata a GPT-OSS</a></p> </li> <li> <p>La versione 120B rimane fuori dalla portata dei PC standard, richiedendo solitamente infrastrutture di classe enterprise.</p> </li> </ul> <p><strong>In sintesi:</strong> Per iniziare è sufficiente un notebook moderno con <strong>16GB di RAM</strong> (o acora meglio un Mackbook o un MacMini), scaricare <strong>LM Studio</strong> e provare un modello come <strong>Mistral</strong> o <strong>Llama 3.1 8B</strong> in versione quantizzata a 4-bit. Se la memoria video non è sufficiente, questi strumenti sposteranno automaticamente parte del lavoro sulla RAM di sistema, sacrificando la velocità per garantire l’esecuzione.</p> <p>Con un Mac Mini, equipaggiato con processore AppleSilicon serie M, si ottiene un rapporto prezzo/prestazioni sicuramente interessante e la possibilità di eseguire modelli di dimensione fino a 20B.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/2025-07-11-Online_latex_editor/"></a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/progetto-creare-un-libro/">Creare un libro in famiglia. Molto più di un gioco</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Dove_pubblicare_tutorial_e_dispense_informatica/">Dove pubblicare i tuoi tutorial e dispense di informatica</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/hosting-website-github/">Hosting a Personal Website on GitHub Pages</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/Differential_Privacy/">Differential Privacy</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Emanuele Buchicchio. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>