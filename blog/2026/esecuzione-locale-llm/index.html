<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Eseguire un assistente AI basato su LLM in locale | Emanuele Buchicchio </title> <meta name="author" content="Emanuele Buchicchio"> <meta name="description" content="L'esecuzione di modelli di linguaggio di grandi dimensioni (LLM) in locale è diventata oggi una realtà pratica per utenti privati grazie alla maturazione di software intuitivi e a tecniche di compressione avanzate"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://emanubuc.github.io/blog/2026/esecuzione-locale-llm/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Emanuele</span> Buchicchio </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Eseguire un assistente AI basato su LLM in locale</h1> <p class="post-meta"> Created on January 01, 2026 </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> LLM</a>   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   ·   <a href="/blog/category/ai"> <i class="fa-solid fa-tag fa-sm"></i> AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>L’esecuzione di modelli di linguaggio di grandi dimensioni (LLM) in locale è diventata oggi una realtà pratica per utenti privati grazie alla maturazione di software intuitivi e a tecniche di compressione avanzate. Gestire un’IA sul proprio PC o notebook offre vantaggi determinanti come la privacy assoluta dei dati, l’accesso offline, l’assenza di costi di abbonamento e la possibilità di personalizzazione avanzata.</p> <p>Questa transizione è stata resa possibile dalla convergenza di tre fattori critici: la disponibilità di modelli open-weight di classe enterprise, come le serie Llama, Mistral e Qwen; lo sviluppo di tecniche di compressione sofisticate note come quantizzazione; e la democratizzazione dell’hardware ad alte prestazioni, incluse le GPU consumer e i chip con memoria unificata.</p> <p>L’esecuzione locale di LLM è diventata un’imperativo strategico per le organizzazioni che intendono mantenere la sovranità sui propri dati, ottimizzare i costi operativi e garantire la continuità del servizio in assenza di connettività internet. Per supportare carichi di lavoro elevati in contesti aziendali (molti utenti e numero richieste di inferenza anche contemporanee) è ncessario disporre di compenti hardware specifiche, dove la larghezza di banda della memoria e la VRAM della GPU risultano cruciali per garantire prestazioni accettabili. In questo articolo ci occupermo però solo dello scenario di utilizzo “personale” (singolo utente) e per un utilizzo personale un notebook recente con 16/32GB di RAM o un Mac Mini possono esserre già sufficienti.</p> <h2 id="1-requisiti-hardware">1. Requisiti Hardware</h2> <p>Sebbene gli LLM siano nati per girare su server potenti, l’hardware consumer attuale (2025/2026) può gestire modelli di dimensioni contenute con prestazioni soddisfacenti.</p> <ul> <li> <strong>Memoria RAM/VRAM:</strong> È il fattore più critico. Se il modello risiede interamente nella memoria video (<strong>VRAM</strong>) della scheda grafica (GPU), l’esecuzione è da 5 a 20 volte più rapida rispetto alla RAM di sistema. <ul> <li> <strong>Configurazione Base (8GB RAM):</strong> Può far girare piccoli modelli da 1B-3B parametri.</li> <li> <strong>Configurazione Consigliata (16GB+ RAM):</strong> Il punto di equilibrio per modelli da <strong>7B-8B</strong> parametri, come Llama 3 o Mistral.</li> <li> <strong>Apple Silicon (M1/M2/M3/M4):</strong> Questi chip offrono un vantaggio unico grazie alla <strong>Memoria Unificata</strong>, che permette alla GPU di accedere a tutta la RAM di sistema come se fosse VRAM.</li> </ul> </li> <li> <strong>Scheda Video (GPU):</strong> Le schede <strong>NVIDIA RTX</strong> (serie 3000/4000/5000) sono lo standard grazie al supporto CUDA. Una GPU con <strong>8GB-12GB di VRAM</strong> (come la RTX 3060 o 4060) è ideale per un’esperienza fluida con modelli medi.</li> <li> <strong>Processore (CPU):</strong> Se non si dispone di una GPU dedicata, è necessaria una CPU moderna (almeno 11ª gen Intel o AMD Zen 4) con supporto alle istruzioni <strong>AVX-512</strong> per accelerare i calcoli.</li> </ul> <h2 id="2-limportanza-della-quantizzazione">2. L’importanza della Quantizzazione</h2> <p>Senza compressione, un modello da 7 miliardi di parametri richiederebbe circa 14-16 GB di VRAM, risultando inaccessibile per hardware standard. La <strong>quantizzazione</strong> riduce la precisione numerica dei pesi del modello (es. da 16-bit a 4-bit) senza comprometterne drasticamente l’intelligenza.</p> <p>Il formato <strong>Q4_K_M</strong> (4-bit) è considerato il <strong>“gold standard”</strong> per l’uso locale: riduce le dimensioni del modello del 75%, permettendo a un modello 7B di girare su soli 5-6 GB di VRAM con una perdita di qualità trascurabile.</p> <p>Il tema dei formati dei modelli e della quantizzaizone è ampio e lo approfondiremo in un altra occasione.</p> <h2 id="3-strumenti-software-principali">3. Strumenti Software Principali</h2> <p>Esistono diverse piattaforme che riducono il processo di installazione e configuraizone a pochi click. La piattaforma di esecuzione più performante per un utilizzo personale è sicuramente <a href="https://github.com/ggml-org/llama.cpp" rel="external nofollow noopener" target="_blank">Llama.cpp</a>.</p> <p>Llama.cpp è un’implementazione in puro C/C++ creata da Georgi Gerganov. La sua importanza risiede nel fatto che è stata la prima a dimostrare che i modelli linguistici di grandi dimensioni potevano girare in modo efficiente su hardware consumer (sia CPU che GPU) grazie a tecniche avanzate di quantizzazione e gestione della memoria. Supporta una vasta gamma di accelerazioni, tra cui CUDA (NVIDIA), Metal (Apple Silicon), ROCm (AMD) e Vulkan.</p> <p>Molti (tutti?) strumenti popolari come Ollama, LM Studio e Jan utilizzano llama.cpp come motore di inferenza principale. Llama.cpp dispone però anche di una sua interfaccia a riga di comando, di una interfaccia web in stile ChatGPT e di un server che ne permette l’utilizzo da parte di altri client. Ad esempio per eseguire GPT-OSS su un Mac Mini con 16GB</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>llama-server -hf ggml-org/gpt-oss-20b-GGUF --n-cpu-moe 12 -c 32768 --jinja --no-mmap
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/llama-gpt-oss-480.webp 480w,/assets/img/llama-gpt-oss-800.webp 800w,/assets/img/llama-gpt-oss-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/llama-gpt-oss.png" width="100%" height="auto" title="Optional hover title" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 1: llama-server in esecuzione.</figcaption> </figure> <p>Ovviamene il “peso” dell’esecuzione del modello si fa sentire e non permette di eseguire contemporanemante altre attività pesanti però il sistema è utilizzabile.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gpt-oss-web-interface-480.webp 480w,/assets/img/gpt-oss-web-interface-800.webp 800w,/assets/img/gpt-oss-web-interface-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/gpt-oss-web-interface.png" width="100%" height="auto" title="Optional hover title" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 2: Interfaccia web in stile Chat esposta da llama-server.</figcaption> </figure> <p>Altri strumenti molto popolari, che aggiungono un interfaccia utente più elaborata sopra Llama.cpp sacrificando però le prestazioni compessive. Ad esempio, sul Mac Mini M2 16GB con Llama.cpp il modello GPT-OSS 20B è utilizzabile, mentre con altri strumenti riesce a malapena ad avviarsi e poi è troppo lento per un qualsiasi utilizzo pratico interattivo.</p> <p>Anche se personalmente consiglio di usare Llama.cpp, alcuni strumenti popolari sono:</p> <ul> <li> <strong>LM Studio:</strong> Forse la soluzione più accessibile per utenti non tecnici. Offre un’interfaccia grafica completa, un motore di ricerca integrato per scaricare modelli da Hugging Face e permette di visualizzare l’uso delle risorse in tempo reale.</li> <li> <strong>Ollama:</strong> Estremamente popolare tra gli utenti più esperti e gli sviluppatori. Funziona tramite riga di comando (CLI) ma può essere abbinato a interfacce web grafiche come <strong>OpenWebUI</strong> per un’esperienza simile a ChatGPT.</li> <li> <strong>Jan.ai:</strong> Un’alternativa <strong>open-source</strong> focalizzata sulla privacy che funziona interamente offline. Ha un’interfaccia minimalista e supporta l’estensione tramite plugin.</li> <li> <strong>GPT4All:</strong> Ottimizzato per girare su hardware modesto (anche solo CPU). Include la funzione <strong>LocalDocs</strong>, che permette di “chattare” con i propri documenti locali (PDF, Word) in modo semplice.</li> </ul> <h2 id="4-modelli-consigliati-per-hardware-standard">4. Modelli Consigliati per Hardware Standard</h2> <p>Per un PC o notebook standard, si suggerisce di iniziare con queste famiglie di modelli (versioni quantizzate):</p> <ul> <li> <strong>Llama 3.2 (1B / 3B):</strong> Ultra-leggeri, ideali per notebook con poca RAM.</li> <li> <strong>Mistral (7B) / Llama 3.1 (8B):</strong> I modelli più versatili per compiti generali e scrittura.</li> <li> <strong>Qwen 2.5/3 (0.5B - 7B):</strong> Eccellenti per il coding e compiti multilingue.</li> <li> <strong>Phi-3/4 (Mini):</strong> Modelli piccoli di Microsoft con capacità di ragionamento sorprendenti per la loro dimensione.</li> </ul> <h3 id="openai-gpt-oss">OpenAI GPT-OSS</h3> <p>OpenAI GPT-OSS è un’aggiunta fondamentale al panorama dei modelli locali, rappresentando il primo rilascio open-weight significativo di OpenAI dopo anni, avvenuto nell’agosto 2025. Il modello è disponibile principalmente nelle versioni 20B e 120B. La variante 120B, in particolare, offre prestazioni di livello GPT-4 ed eccelle in compiti di ragionamento avanzato, tool calling e workflow agentici complessi.</p> <ul> <li> <p>La versione 20B è quella più indicata per hardware locale avanzato, ma richiede comunque una configurazione di fascia alta con almeno 32GB di RAM/VRAM per girare correttamente. <em>Sul mio Mac Mini Apple Silicon M2 con 16GB di RAM condivisa</em> si riesce ad eseguire usando LLAMA-CPP <a href="https://github.com/ggml-org/llama.cpp/discussions/15396" rel="external nofollow noopener" target="_blank">seguendo la guida dedicata a GPT-OSS</a></p> </li> <li> <p>La versione 120B rimane fuori dalla portata dei PC standard, richiedendo solitamente infrastrutture di classe enterprise.</p> </li> </ul> <p><strong>In sintesi:</strong> Per iniziare è sufficiente un notebook moderno con <strong>16GB di RAM</strong> (o acora meglio un Mackbook o un MacMini), scaricare <strong>LM Studio</strong> e provare un modello come <strong>Llama 3.1 8B</strong> in versione quantizzata a 4-bit. Se la memoria video non è sufficiente, questi strumenti sposteranno automaticamente parte del lavoro sulla RAM di sistema, sacrificando la velocità per garantire l’esecuzione.</p> <p>Con un Mac Mini, equipaggiato con processore AppleSilicon serie M, si ottiene un rapporto prezzo/prestazioni sicuramente interessante e la possibilità di eseguire modelli di dimensione fino a 20B.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/2025-07-11-Online_latex_editor/"></a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/progetto-creare-un-libro/">Creare un libro in famiglia. Molto più di un gioco</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Dove_pubblicare_tutorial_e_dispense_informatica/">Dove pubblicare i tuoi tutorial e dispense di informatica</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/hosting-website-github/">Hosting a Personal Website on GitHub Pages</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/Differential_Privacy/">Differential Privacy</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Emanuele Buchicchio. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>